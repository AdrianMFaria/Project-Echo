{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview\n",
    "\n",
    "Author: stephankokkas\n",
    "\n",
    "This notebook defines a pipeline that tasks an input directory of audio files and converts them to images using mel-spectrogram transofrmation and preprocessing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version     :  3.10.0\n",
      "TensorFlow Version :  2.11.0\n"
     ]
    }
   ],
   "source": [
    "# disable warnings to tidy up output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# some basic libraries \n",
    "from platform import python_version\n",
    "#import pandas as pd\n",
    "#import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "import torch\n",
    "from IPython.display import Audio\n",
    "import pandas as pd\n",
    "\n",
    "# plot support\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tensorflow support\n",
    "import tensorflow as tf\n",
    "#import tensorflow_transform as tft\n",
    "import tensorflow_io as tfio\n",
    "#from tensorflow.contrib.framework.python.ops import audio_ops\n",
    "\n",
    "# scipy\n",
    "import scipy\n",
    "from pydub import AudioSegment, effects\n",
    "\n",
    "# turn off tensorflow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# turn off absl warnings\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "# print system information\n",
    "print('Python Version     : ', python_version())\n",
    "print('TensorFlow Version : ', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below code adapted from:\n",
    "# https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(123)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "random.seed(123)\n",
    "\n",
    "# The below set_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "\n",
    "This pipeline will go through a root directory and find all the audio files that exist and are of accepted format. Then, depending on the params set, it with normalise, trim and split the data. Please ensure you specify the self.DATASET_PATH with the directory of the data, and the self._SET_OUTPUT_DIR with the location you want the output files to end up in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class raw_file_pre_processing():\n",
    "    def __init__(self) -> None:\n",
    "        self.CLIP_LENGTH   = 5000   # only look at 5000 milliseconds of clip at the start of loaded audio file\n",
    "        self.BITRATE = \"160k\"        # all the samples are converted to bit rate of 32000 (Samples/Second)\n",
    "        self.labels = []\n",
    "        self.raw_dirs = {}\n",
    "        self.dataset = pd.DataFrame(columns=['Label', 'FileName', 'FileType', 'Directory'])\n",
    "        self.TARGET_FORMAT = 'flac'\n",
    "        self.ACCEPTED_FORMAT = ['.mp3', '.flac', '.aiff', '.mp4', '.m4a', '.wav', '.ogg']\n",
    "        self.CLEAN_DIR_ = True\n",
    "        self.TRAIN_SPLIT = 0.8\n",
    "        self.TEST_SPLIT = 0.1\n",
    "        self.VALIDATION_SPLIT = 0.1\n",
    "        self.split_file_names = {}\n",
    "\n",
    "        # Please make sure that if you are on a Mac, that your path resebles the below example path\n",
    "        # if you are on windows, please ensure that your path looks like this:\n",
    "\n",
    "        # WIN\n",
    "        #self.DATASET_PATH = 'C:\\\\Users\\\\steph\\\\Documents\\\\birdclef2022\\\\'\n",
    "        # MAC\n",
    "        self.DATASET_PATH  = '/Users/stephankokkas/Downloads/project-echo-data/'\n",
    "        \n",
    "        # make sure this is in the same format for either window or mac\n",
    "        #WIN\n",
    "        #self._SET_OUTPUT_DIR = 'C:\\\\Users\\\\steph\\\\Downloads\\\\'\n",
    "        #MAC\n",
    "        self._SET_OUTPUT_DIR = '/Users/stephankokkas/Downloads/'\n",
    "        \n",
    "        \n",
    "        if '/' in self.DATASET_PATH:\n",
    "            self.DATASET_PATH = os.path.join('/', *self.DATASET_PATH.split('/'))\n",
    "            self.OUTPUT_DIR = os.path.join('/', *self._SET_OUTPUT_DIR.split('/'), 'OUTPUT_raw_flac')\n",
    "            if not os.path.exists(self.OUTPUT_DIR):\n",
    "                os.mkdir(self.OUTPUT_DIR)\n",
    "        elif '\\\\' in str(self.DATASET_PATH):\n",
    "            self.DATASET_PATH = str(os.path.join(*self.DATASET_PATH.split('\\\\'))).replace(':', ':\\\\')\n",
    "            self.OUTPUT_DIR = str(os.path.join(*self._SET_OUTPUT_DIR.split('\\\\'), 'OUTPUT_raw_flac')).replace(':', ':\\\\')\n",
    "            if not os.path.exists(self.OUTPUT_DIR):\n",
    "                os.makedirs(self.OUTPUT_DIR)   \n",
    "                \n",
    "\n",
    "        self.TRAIN_DIR = os.path.join(str(self.OUTPUT_DIR).replace('OUTPUT_raw_flac', ''), 'TRAIN_raw_flac')\n",
    "        self.TEST_DIR = os.path.join(str(self.OUTPUT_DIR).replace('OUTPUT_raw_flac', ''), 'TEST_raw_flac')\n",
    "        self.VALIDATION_DIR = os.path.join(str(self.OUTPUT_DIR).replace('OUTPUT_raw_flac', ''), 'VALIDATION_raw_flac')\n",
    "\n",
    "        def clean_dir(self):\n",
    "            if os.path.exists(self.OUTPUT_DIR):\n",
    "                shutil.rmtree(self.OUTPUT_DIR)\n",
    "            if os.path.exists(self.TRAIN_DIR):\n",
    "                shutil.rmtree(self.TRAIN_DIR)\n",
    "            if os.path.exists(self.TEST_DIR):\n",
    "                shutil.rmtree(self.TEST_DIR)\n",
    "            if os.path.exists(self.VALIDATION_DIR):\n",
    "                shutil.rmtree(self.VALIDATION_DIR)\n",
    "        if self.CLEAN_DIR_: \n",
    "            clean_dir(self)\n",
    "\n",
    "    def handle_duplicate_files(self) -> bool:\n",
    "        try:\n",
    "            for root, dir, files in os.walk(self.DATASET_PATH):\n",
    "                for _class_ in dir:\n",
    "                    _file_ = [f.split('.')[0] for f in listdir(os.path.join(root, _class_)) if isfile(join(os.path.join(root, _class_), f))]\n",
    "                    _set_ = set([x for x in _file_ if _file_.count(x) > 1])\n",
    "\n",
    "                    if len(_set_) > 0:\n",
    "                        print(f'Class {_class_} has the following duplicates: {_set_}. Will remove dupliactes...')\n",
    "                        for _elem_ in _set_:\n",
    "                            for dir_file in [f for f in listdir(os.path.join(root, _class_)) if isfile(join(os.path.join(root, _class_), f))]:\n",
    "                                if _elem_ in dir_file:\n",
    "                                    os.remove(os.path.join(root, _class_, dir_file))      \n",
    "                break\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Terminating...')\n",
    "            return False\n",
    "                    \n",
    "    def get_raw_file_paths(self):\n",
    "        if self.handle_duplicate_files():\n",
    "            print(f'Looking for files... acceptable formats include: {self.ACCEPTED_FORMAT}')\n",
    "            for root, dir, files in os.walk(self.DATASET_PATH):\n",
    "                if dir == []:\n",
    "                    tmp_lable = os.path.split(root)[-1]\n",
    "                    tmp_file_dir = []\n",
    "                    tmp_filename_ = []\n",
    "                    for file in files:\n",
    "                        for ext in self.ACCEPTED_FORMAT:\n",
    "                            if ext in str(file):\n",
    "                                tmp_file_dir.append(os.path.join(root, file))\n",
    "                                tmp_filename_.append(str(os.path.split(os.path.join(root, file))[-1]).split('.')[0])\n",
    "\n",
    "                            \n",
    "                    self.raw_dirs.update({tmp_lable:tmp_file_dir})\n",
    "                    self.split_file_names.update({tmp_lable:tmp_filename_})\n",
    "\n",
    "            for key in self.raw_dirs:\n",
    "                print(f'FOUND: {key} -> {len(self.raw_dirs[key])}')\n",
    "\n",
    "    def audio_preprocessing(self, TRIM_AUDIO:bool = False, \n",
    "                                  NORM_AUDIO:bool = False):\n",
    "        print('\\nConvering audio files....\\n')\n",
    "        if not os.path.exists(self.OUTPUT_DIR):\n",
    "            os.makedirs(self.OUTPUT_DIR)\n",
    "\n",
    "        \n",
    "        for key, item in self.split_file_names.items():\n",
    "            train = item[int(len(item) * .00) : int(len(item) * self.TRAIN_SPLIT)]\n",
    "            vali = item[int(len(item) * self.TRAIN_SPLIT) : int(len(item) * (self.TRAIN_SPLIT + self.VALIDATION_SPLIT))]\n",
    "            test = item[int(len(item) * (self.TRAIN_SPLIT + self.VALIDATION_SPLIT)) : int(len(item) * 1.00)]\n",
    "            self.split_file_names.update({key: [train, vali, test]})\n",
    "\n",
    "        for key, item in self.raw_dirs.items():\n",
    "            print(f'Converting {key} data ->> ...')\n",
    "            tmp_dir_key = os.path.join(self.OUTPUT_DIR, key)\n",
    "            if not os.path.exists(tmp_dir_key):\n",
    "                os.makedirs(tmp_dir_key)\n",
    "\n",
    "            for dir in item:\n",
    "                try:\n",
    "                    # read file\n",
    "                    tmp_file_name = os.path.split(dir)[-1].split('.')[0]\n",
    "                    raw_sound = AudioSegment.from_file(dir, format=dir.split('.')[-1])\n",
    "\n",
    "                    if NORM_AUDIO:\n",
    "                        # normalise file\n",
    "                        raw_sound = effects.normalize(raw_sound)\n",
    "\n",
    "                    # trim file\n",
    "                    if TRIM_AUDIO:\n",
    "                        arr_split_file = [raw_sound[idx:idx + self.CLIP_LENGTH] for idx in range(0, len(raw_sound), self.CLIP_LENGTH)]             \n",
    "                        for count_sample, sample in enumerate(arr_split_file):\n",
    "                            # padding audio < 5s\n",
    "                            if len(sample) < self.CLIP_LENGTH:\n",
    "                                silence = AudioSegment.silent(duration=((self.CLIP_LENGTH-len(sample))))\n",
    "                                sample = sample + silence  # Adding silence after the audio\n",
    "\n",
    "                            # export raw file\n",
    "                            tmp_raw_new_dir = os.path.join(tmp_dir_key, tmp_file_name + '_raw_trim_sample_' + str(count_sample) + '.' + self.TARGET_FORMAT)\n",
    "                            sample.export(tmp_raw_new_dir, format=self.TARGET_FORMAT, bitrate=self.BITRATE)\n",
    "\n",
    "                            new_row = pd.Series({\"Label\": key,\n",
    "                                        \"FileName\": tmp_file_name + '_raw_trim_sample_' + str(count_sample) + '.' + self.TARGET_FORMAT,\n",
    "                                        \"FileType\": self.TARGET_FORMAT,\n",
    "                                        \"Directory\": tmp_raw_new_dir})\n",
    "                            self.dataset = pd.concat([self.dataset, new_row.to_frame().T], ignore_index=True)\n",
    "                    else:\n",
    "                        tmp_raw_new_dir = os.path.join(tmp_dir_key, tmp_file_name + '_raw_' + '.' + self.TARGET_FORMAT)\n",
    "                        raw_sound.export(tmp_raw_new_dir, format=self.TARGET_FORMAT, bitrate=self.BITRATE)\n",
    "\n",
    "                        new_row = pd.Series({\"Label\": key,\n",
    "                                    \"FileName\": tmp_file_name + '_raw_' + '.' + self.TARGET_FORMAT,\n",
    "                                    \"FileType\": self.TARGET_FORMAT,\n",
    "                                    \"Directory\": tmp_raw_new_dir})\n",
    "                        self.dataset = pd.concat([self.dataset, new_row.to_frame().T], ignore_index=True)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "    def train_test_split_fun(self):\n",
    "        print(f'\\nSplitting data into sub-directories Train, Test and Validation...')\n",
    "\n",
    "        if not os.path.exists(self.TRAIN_DIR):\n",
    "            os.mkdir(self.TRAIN_DIR)\n",
    "        if not os.path.exists(self.TEST_DIR):\n",
    "            os.mkdir(self.TEST_DIR)\n",
    "        if not os.path.exists(self.VALIDATION_DIR):\n",
    "            os.mkdir(self.VALIDATION_DIR)\n",
    "\n",
    "\n",
    "        dict_keys = self.dataset['Label'].value_counts().to_dict()\n",
    "        for key, item in dict_keys.items():\n",
    "            if not os.path.exists(os.path.join(self.TRAIN_DIR, key)):\n",
    "                os.mkdir(os.path.join(self.TRAIN_DIR, key))\n",
    "            if not os.path.exists(os.path.join(self.TEST_DIR, key)):\n",
    "                os.mkdir(os.path.join(self.TEST_DIR, key))\n",
    "            if not os.path.exists(os.path.join(self.VALIDATION_DIR, key)):\n",
    "                os.mkdir(os.path.join(self.VALIDATION_DIR, key))\n",
    "\n",
    "\n",
    "        # self.dataset.to_csv(os.path.join(self.OUTPUT_DIR, 'raw_files.csv'))\n",
    "\n",
    "        for index, row in self.dataset.iterrows():\n",
    "            i = str(row.Directory)\n",
    "            for key, item in self.split_file_names.items():\n",
    "                if str(row.FileName).split('_')[0] in item[0]:\n",
    "                    os.replace(i, i.replace('OUTPUT_raw_flac', 'TRAIN_raw_flac'))\n",
    "                if str(row.FileName).split('_')[0] in item[1]:\n",
    "                    os.replace(i, i.replace('OUTPUT_raw_flac', 'VALIDATION_raw_flac'))\n",
    "                if str(row.FileName).split('_')[0] in item[2]:\n",
    "                    os.replace(i, i.replace('OUTPUT_raw_flac', 'TEST_raw_flac'))\n",
    "\n",
    "        shutil.rmtree(self.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for files... acceptable formats include: ['.mp3', '.flac', '.aiff', '.mp4', '.m4a', '.wav', '.ogg']\n",
      "FOUND: Aegotheles cristatus Australian owlet-nightjar -> 62\n",
      "FOUND: Caligavis chrysops Yellow-faced honeyeater -> 55\n",
      "FOUND: Colluricincla harmonica Grey shrikethrush -> 75\n",
      "FOUND: Ptilotula penicillata White-plumed honeyeater -> 58\n",
      "FOUND: Eopsaltria australis Eastern yellow robin -> 69\n",
      "FOUND: Cervus unicolour Sambar deer -> 13\n",
      "FOUND: Pachycephala rufiventris Rufous whistler -> 76\n",
      "FOUND: Dama dama Fallow Deer -> 33\n",
      "FOUND: Corvus coronoides Australian raven -> 55\n",
      "FOUND: Strepera graculina Pied currawong -> 67\n",
      "FOUND: Alauda arvensis European Skylark -> 54\n",
      "FOUND: vulpes vulpes red fox -> 47\n",
      "FOUND: Rattus norvegicus Brown rat -> 17\n",
      "FOUND: sus scrofa Wild pig -> 63\n",
      "FOUND: Felis Catus Cat -> 200\n",
      "FOUND: Capra hircus Feral goat -> 32\n",
      "\n",
      "This next process will take approx 20 mins for the current bird dataset depending on the speed of your computer\n",
      "\n",
      "Convering audio files....\n",
      "\n",
      "Converting Aegotheles cristatus Australian owlet-nightjar data ->> ...\n",
      "Converting Caligavis chrysops Yellow-faced honeyeater data ->> ...\n",
      "Converting Colluricincla harmonica Grey shrikethrush data ->> ...\n",
      "Converting Ptilotula penicillata White-plumed honeyeater data ->> ...\n",
      "Converting Eopsaltria australis Eastern yellow robin data ->> ...\n",
      "Converting Cervus unicolour Sambar deer data ->> ...\n",
      "Converting Pachycephala rufiventris Rufous whistler data ->> ...\n",
      "Converting Dama dama Fallow Deer data ->> ...\n",
      "Converting Corvus coronoides Australian raven data ->> ...\n",
      "Converting Strepera graculina Pied currawong data ->> ...\n",
      "Converting Alauda arvensis European Skylark data ->> ...\n",
      "Converting vulpes vulpes red fox data ->> ...\n",
      "Converting Rattus norvegicus Brown rat data ->> ...\n",
      "Converting sus scrofa Wild pig data ->> ...\n",
      "Converting Felis Catus Cat data ->> ...\n",
      "Converting Capra hircus Feral goat data ->> ...\n",
      "\n",
      "Splitting data into sub-directories Train, Test and Validation...\n"
     ]
    }
   ],
   "source": [
    "data_preprocessing_pipeline = raw_file_pre_processing()\n",
    "\n",
    "data_preprocessing_pipeline.get_raw_file_paths()\n",
    "\n",
    "print('\\nThis next process will take approx 20 mins for the current bird dataset depending on the speed of your computer')\n",
    "data_preprocessing_pipeline.audio_preprocessing(TRIM_AUDIO=True, NORM_AUDIO=True)\n",
    "data_preprocessing_pipeline.train_test_split_fun()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melspectrogram Pipeline\n",
    "\n",
    "This pipeline will get all the new flac files, convert them to tfio tensors, convert to spectrograms, convert again to mel-spectrograms. It will then save the tensors as .pt files which can be read again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mel_spectrogram_pipeline():\n",
    "    def __init__(self, output_directory) -> None:\n",
    "        self.target_dir = ''\n",
    "        self.labels = []\n",
    "        self.augmented_dirs = {}\n",
    "        self.OUTPUT_DIR = {}\n",
    "        self.ACCEPTED_FORMAT = '.flac'\n",
    "\n",
    "        # do not change, this will be brought across from the previous pipeline\n",
    "        self._SET_OUTPUT_DIR = output_directory\n",
    "        if '/' in self._SET_OUTPUT_DIR:\n",
    "            self.DATASET_PATH = self._SET_OUTPUT_DIR\n",
    "            self.TENSOR_OUTPUT_DIR = os.path.join(self.DATASET_PATH, 'OUTPUT_tensors')\n",
    "        elif '\\\\' in self._SET_OUTPUT_DIR:\n",
    "            self.DATASET_PATH = self._SET_OUTPUT_DIR\n",
    "            self.TENSOR_OUTPUT_DIR = os.path.join(self.DATASET_PATH, 'OUTPUT_tensors')\n",
    "\n",
    "        self.NFFT = 512\n",
    "        self.WINDOW = 512\n",
    "        self.STRIDE = 512\n",
    "        self.SAMPLE_RATE = int(44100/2)\n",
    "        self.MELS = 128\n",
    "        self.FMIN = 0\n",
    "        self.FMAX = int(self.SAMPLE_RATE)/2\n",
    "        self.TOP_DB = 80\n",
    "\n",
    "    \n",
    "    def clean_dirs(self) -> None:\n",
    "        print('Cleaning tensor directory')\n",
    "        if os.path.exists(self.TENSOR_OUTPUT_DIR):\n",
    "            shutil.rmtree(self.TENSOR_OUTPUT_DIR)\n",
    "\n",
    "    def get_output_dir(self) -> None:\n",
    "        print('Finding all pre-processed files')\n",
    "        for root, dir, files in os.walk(self.DATASET_PATH):\n",
    "            if \"TRAIN_raw_flac\" not in dir:\n",
    "                if \"OUTPUT\" not in dir:\n",
    "                    raise ValueError('Cant find any directories with pre-processed data. Looking for OUTPUT or TRAIN, TEST, and VAIDATION')\n",
    "                else:\n",
    "                    self.OUTPUT_DIR.update({\"OUTPUT\": os.path.join(root, \"OUTPUT\")})\n",
    "            else:\n",
    "                if \"TEST_raw_flac\" in dir and \"VALIDATION_raw_flac\" in dir:\n",
    "                    self.OUTPUT_DIR.update({\"TRAIN\": os.path.join(root, \"TRAIN_raw_flac\")})\n",
    "                    self.OUTPUT_DIR.update({\"TEST\": os.path.join(root, \"TEST_raw_flac\")})\n",
    "                    self.OUTPUT_DIR.update({\"VALIDATION\": os.path.join(root, \"VALIDATION_raw_flac\")})\n",
    "            if not self.OUTPUT_DIR:\n",
    "                raise ValueError('Cant find any directories with pre-processed data. Looking for OUTPUT or TRAIN, TEST, and VAIDATION')\n",
    "\n",
    "            print(f'\\nFound the following directories {self.OUTPUT_DIR}\\n')\n",
    "            break\n",
    "            \n",
    "    def get_preprocessed_files(self) -> None:\n",
    "        self.get_output_dir()\n",
    "\n",
    "        for key, item in self.OUTPUT_DIR.items():\n",
    "            for root, dir, files in os.walk(item):\n",
    "                if dir == []:\n",
    "                    tmp_lable = str(os.path.split(root)[-1]) + \"~\" + key\n",
    "                    tmp_file_dir = []\n",
    "                    for file in files:\n",
    "                        if self.ACCEPTED_FORMAT in str(file):\n",
    "                            tmp_file_dir.append(os.path.join(root, file))\n",
    "                            \n",
    "                    self.augmented_dirs.update({tmp_lable:tmp_file_dir})\n",
    "\n",
    "        for key in self.augmented_dirs:\n",
    "            print(f'FOUND: {key} -> {len(self.augmented_dirs[key])}')\n",
    "\n",
    "    def generate_mel_spectrograms(self, MEL_SPECTRO:bool = False, \n",
    "                                        SHOW_PLOT:bool = False, \n",
    "                                        FREQ_MASK:bool = False, \n",
    "                                        TIME_MASK:bool = False,\n",
    "                                        TORCH_EXPORT:bool = False, \n",
    "                                        TFIO_EXPORT:bool = False) -> None:\n",
    "                                        \n",
    "        print(f'\\nGenerating tensors... \\n')\n",
    "        for key, item in self.augmented_dirs.items():\n",
    "            for dir in item:\n",
    "                tmp_label = key.split('~')[0]\n",
    "                tmp_set = key.split('~')[1]\n",
    "                if not os.path.exists(os.path.join(self.TENSOR_OUTPUT_DIR)):\n",
    "                    os.mkdir(os.path.join(self.TENSOR_OUTPUT_DIR))\n",
    "                if not os.path.exists(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set)):\n",
    "                    os.mkdir(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set))\n",
    "                if not os.path.exists(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set, tmp_label)):\n",
    "                    os.mkdir(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set, tmp_label))\n",
    "       \n",
    "                file_contents=tf.io.read_file(dir)\n",
    "                tmp_audio_t = tfio.audio.decode_flac(input=file_contents, dtype=tf.int16)\n",
    "                tmp_audio_t = tf.cast(tfio.audio.resample(tmp_audio_t, tfio.audio.AudioIOTensor(dir)._rate.numpy(), self.SAMPLE_RATE), tf.float32)\n",
    "\n",
    "                # Convert to spectrogram\n",
    "                spectrogram = tfio.audio.spectrogram(\n",
    "                    tmp_audio_t[:, 0], nfft=self.NFFT, window=self.WINDOW, stride=self.STRIDE)\n",
    "\n",
    "                if SHOW_PLOT:\n",
    "                    plt.figure()\n",
    "                    plt.imshow(tf.math.log(spectrogram).numpy())\n",
    "\n",
    "                if MEL_SPECTRO:\n",
    "                    # # Convert to mel-spectrogram\n",
    "                    mel_spectrogram = tfio.audio.melscale(\n",
    "                        spectrogram, rate=self.SAMPLE_RATE, mels=self.MELS, fmin=self.FMIN, fmax=self.FMAX)\n",
    "\n",
    "                    if SHOW_PLOT:\n",
    "                        plt.figure()\n",
    "                        plt.imshow(tf.math.log(mel_spectrogram).numpy())\n",
    "\n",
    "                    if TORCH_EXPORT:\n",
    "                        torch.save(mel_spectrogram, str(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set, tmp_label, os.path.split(dir)[-1].split('.')[0])) + '_raw_mel_spectrogram.pt')\n",
    "                    if TFIO_EXPORT:\n",
    "                        tf.io.write_file(str(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set, tmp_label, os.path.split(dir)[-1].split('.')[0])) + '_raw_mel_spectrogram.pt', tf.io.serialize_tensor(mel_spectrogram))\n",
    "\n",
    "                    if FREQ_MASK and \"TEST\" not in dir and \"VALIDATION\" not in dir:\n",
    "                        freq_mask = tfio.audio.freq_mask(mel_spectrogram, param=10)\n",
    "                        if TORCH_EXPORT:\n",
    "                            torch.save(freq_mask, str(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set, tmp_label, os.path.split(dir)[-1].split('.')[0])) + '_freq_mask_mel_spectrogram.pt')\n",
    "                        if TFIO_EXPORT:\n",
    "                            tf.io.write_file(str(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set, tmp_label, os.path.split(dir)[-1].split('.')[0])) + '_freq_mask_mel_spectrogram.pt', tf.io.serialize_tensor(mel_spectrogram))\n",
    "                    \n",
    "                    if TIME_MASK and \"TEST\" not in dir and \"VALIDATION\" not in dir:\n",
    "                        time_mask = tfio.audio.time_mask(mel_spectrogram, param=10)\n",
    "                        if TORCH_EXPORT:\n",
    "                            torch.save(time_mask, str(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set, tmp_label, os.path.split(dir)[-1].split('.')[0])) + '_time_mask_mel_spectrogram.pt')\n",
    "                        if TFIO_EXPORT:\n",
    "                            tf.io.write_file(str(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set, tmp_label, os.path.split(dir)[-1].split('.')[0])) + '_time_mask_mel_spectrogram.pt', tf.io.serialize_tensor(mel_spectrogram))\n",
    "\n",
    "        print(\"\\nTensors complete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning tensor directory\n",
      "Finding all pre-processed files\n",
      "\n",
      "Found the following directories {'TRAIN': '/Users/stephankokkas/Downloads/TRAIN_raw_flac', 'TEST': '/Users/stephankokkas/Downloads/TEST_raw_flac', 'VALIDATION': '/Users/stephankokkas/Downloads/VALIDATION_raw_flac'}\n",
      "\n",
      "FOUND: Aegotheles cristatus Australian owlet-nightjar~TRAIN -> 627\n",
      "FOUND: Caligavis chrysops Yellow-faced honeyeater~TRAIN -> 3704\n",
      "FOUND: Colluricincla harmonica Grey shrikethrush~TRAIN -> 3462\n",
      "FOUND: Ptilotula penicillata White-plumed honeyeater~TRAIN -> 2110\n",
      "FOUND: Eopsaltria australis Eastern yellow robin~TRAIN -> 3457\n",
      "FOUND: Cervus unicolour Sambar deer~TRAIN -> 79\n",
      "FOUND: Pachycephala rufiventris Rufous whistler~TRAIN -> 3905\n",
      "FOUND: Dama dama Fallow Deer~TRAIN -> 55\n",
      "FOUND: Corvus coronoides Australian raven~TRAIN -> 2439\n",
      "FOUND: Strepera graculina Pied currawong~TRAIN -> 3403\n",
      "FOUND: Alauda arvensis European Skylark~TRAIN -> 0\n",
      "FOUND: vulpes vulpes red fox~TRAIN -> 75\n",
      "FOUND: Rattus norvegicus Brown rat~TRAIN -> 75\n",
      "FOUND: sus scrofa Wild pig~TRAIN -> 173\n",
      "FOUND: Felis Catus Cat~TRAIN -> 0\n",
      "FOUND: Capra hircus Feral goat~TRAIN -> 23\n",
      "FOUND: Aegotheles cristatus Australian owlet-nightjar~TEST -> 98\n",
      "FOUND: Caligavis chrysops Yellow-faced honeyeater~TEST -> 238\n",
      "FOUND: Colluricincla harmonica Grey shrikethrush~TEST -> 402\n",
      "FOUND: Ptilotula penicillata White-plumed honeyeater~TEST -> 131\n",
      "FOUND: Eopsaltria australis Eastern yellow robin~TEST -> 630\n",
      "FOUND: Cervus unicolour Sambar deer~TEST -> 28\n",
      "FOUND: Pachycephala rufiventris Rufous whistler~TEST -> 665\n",
      "FOUND: Dama dama Fallow Deer~TEST -> 3\n",
      "FOUND: Corvus coronoides Australian raven~TEST -> 836\n",
      "FOUND: Strepera graculina Pied currawong~TEST -> 156\n",
      "FOUND: Alauda arvensis European Skylark~TEST -> 0\n",
      "FOUND: vulpes vulpes red fox~TEST -> 10\n",
      "FOUND: Rattus norvegicus Brown rat~TEST -> 3\n",
      "FOUND: sus scrofa Wild pig~TEST -> 24\n",
      "FOUND: Felis Catus Cat~TEST -> 0\n",
      "FOUND: Capra hircus Feral goat~TEST -> 2\n",
      "FOUND: Aegotheles cristatus Australian owlet-nightjar~VALIDATION -> 121\n",
      "FOUND: Caligavis chrysops Yellow-faced honeyeater~VALIDATION -> 341\n",
      "FOUND: Colluricincla harmonica Grey shrikethrush~VALIDATION -> 232\n",
      "FOUND: Ptilotula penicillata White-plumed honeyeater~VALIDATION -> 161\n",
      "FOUND: Eopsaltria australis Eastern yellow robin~VALIDATION -> 534\n",
      "FOUND: Cervus unicolour Sambar deer~VALIDATION -> 3\n",
      "FOUND: Pachycephala rufiventris Rufous whistler~VALIDATION -> 523\n",
      "FOUND: Dama dama Fallow Deer~VALIDATION -> 0\n",
      "FOUND: Corvus coronoides Australian raven~VALIDATION -> 439\n",
      "FOUND: Strepera graculina Pied currawong~VALIDATION -> 290\n",
      "FOUND: Alauda arvensis European Skylark~VALIDATION -> 0\n",
      "FOUND: vulpes vulpes red fox~VALIDATION -> 7\n",
      "FOUND: Rattus norvegicus Brown rat~VALIDATION -> 0\n",
      "FOUND: sus scrofa Wild pig~VALIDATION -> 19\n",
      "FOUND: Felis Catus Cat~VALIDATION -> 0\n",
      "FOUND: Capra hircus Feral goat~VALIDATION -> 4\n",
      "\n",
      "This process will take approx 5-10 mins to complete depending on the power of your PC\n",
      "\n",
      "Generating tensors... \n",
      "\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n",
      "Aegotheles cristatus Australian owlet-nightjar~TRAIN\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [154], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m data_spectro_pipeline\u001b[39m.\u001b[39mget_preprocessed_files()\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mThis process will take approx 5-10 mins to complete depending on the power of your PC\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m data_spectro_pipeline\u001b[39m.\u001b[39;49mgenerate_mel_spectrograms(MEL_SPECTRO\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, SHOW_PLOT\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, FREQ_MASK\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, TIME_MASK\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, TORCH_EXPORT\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, TFIO_EXPORT\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn [153], line 91\u001b[0m, in \u001b[0;36mmel_spectrogram_pipeline.generate_mel_spectrograms\u001b[0;34m(self, MEL_SPECTRO, SHOW_PLOT, FREQ_MASK, TIME_MASK, TORCH_EXPORT, TFIO_EXPORT)\u001b[0m\n\u001b[1;32m     89\u001b[0m file_contents\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mread_file(\u001b[39mdir\u001b[39m)\n\u001b[1;32m     90\u001b[0m tmp_audio_t \u001b[39m=\u001b[39m tfio\u001b[39m.\u001b[39maudio\u001b[39m.\u001b[39mdecode_flac(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39mfile_contents, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint16)\n\u001b[0;32m---> 91\u001b[0m tmp_audio_t \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(tfio\u001b[39m.\u001b[39;49maudio\u001b[39m.\u001b[39;49mresample(tmp_audio_t, tfio\u001b[39m.\u001b[39;49maudio\u001b[39m.\u001b[39;49mAudioIOTensor(\u001b[39mdir\u001b[39;49m)\u001b[39m.\u001b[39;49m_rate\u001b[39m.\u001b[39;49mnumpy(), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSAMPLE_RATE), tf\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     93\u001b[0m \u001b[39m# Convert to spectrogram\u001b[39;00m\n\u001b[1;32m     94\u001b[0m spectrogram \u001b[39m=\u001b[39m tfio\u001b[39m.\u001b[39maudio\u001b[39m.\u001b[39mspectrogram(\n\u001b[1;32m     95\u001b[0m     tmp_audio_t[:, \u001b[39m0\u001b[39m], nfft\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mNFFT, window\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWINDOW, stride\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSTRIDE)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow_io/python/ops/audio_ops.py:462\u001b[0m, in \u001b[0;36mresample\u001b[0;34m(input, rate_in, rate_out, name)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(i):\n\u001b[1;32m    458\u001b[0m     \u001b[39mreturn\u001b[39;00m core_ops\u001b[39m.\u001b[39mio_audio_resample(\n\u001b[1;32m    459\u001b[0m         i, rate_in\u001b[39m=\u001b[39mrate_in, rate_out\u001b[39m=\u001b[39mrate_out, name\u001b[39m=\u001b[39mname\n\u001b[1;32m    460\u001b[0m     )\n\u001b[0;32m--> 462\u001b[0m value \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mvectorized_map(f, \u001b[39minput\u001b[39;49m)\n\u001b[1;32m    464\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mg1\u001b[39m():\n\u001b[1;32m    465\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39msqueeze(value, [\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:565\u001b[0m, in \u001b[0;36mvectorized_map\u001b[0;34m(fn, elems, fallback_to_while_loop, warn)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m   batch_size \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(static_first_dims)\n\u001b[0;32m--> 565\u001b[0m \u001b[39mreturn\u001b[39;00m pfor(\n\u001b[1;32m    566\u001b[0m     loop_fn,\n\u001b[1;32m    567\u001b[0m     batch_size,\n\u001b[1;32m    568\u001b[0m     fallback_to_while_loop\u001b[39m=\u001b[39;49mfallback_to_while_loop,\n\u001b[1;32m    569\u001b[0m     warn\u001b[39m=\u001b[39;49mwarn)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:214\u001b[0m, in \u001b[0;36mpfor\u001b[0;34m(loop_fn, iters, fallback_to_while_loop, parallel_iterations, warn)\u001b[0m\n\u001b[1;32m    211\u001b[0m     def_function\u001b[39m.\u001b[39mrun_functions_eagerly(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    212\u001b[0m   f \u001b[39m=\u001b[39m def_function\u001b[39m.\u001b[39mfunction(f)\n\u001b[0;32m--> 214\u001b[0m outputs \u001b[39m=\u001b[39m f()\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m functions_run_eagerly \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m   def_function\u001b[39m.\u001b[39mrun_functions_eagerly(functions_run_eagerly)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:928\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    926\u001b[0m   \u001b[39m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[1;32m    927\u001b[0m   initializers \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 928\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwds, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[1;32m    929\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    930\u001b[0m   \u001b[39m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[1;32m    931\u001b[0m   \u001b[39m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[1;32m    932\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:749\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph \u001b[39m=\u001b[39m lifted_initializer_graph\n\u001b[1;32m    747\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_deleter \u001b[39m=\u001b[39m FunctionDeleter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph)\n\u001b[1;32m    748\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 749\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn    \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    750\u001b[0m     \u001b[39m.\u001b[39;49m_get_concrete_function_internal_garbage_collected(\n\u001b[1;32m    751\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    753\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[1;32m    754\u001b[0m   \u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:162\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m--> 162\u001b[0m   concrete_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_concrete_function(args, kwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:157\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m   args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n\u001b[1;32m    155\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 157\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:360\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m   \u001b[39m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[1;32m    358\u001b[0m   args, kwargs \u001b[39m=\u001b[39m generalized_func_key\u001b[39m.\u001b[39m_placeholder_value()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_concrete_function(args, kwargs)\n\u001b[1;32m    362\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_capture_func_lib  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m# Maintain the list of all captures\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:284\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[1;32m    280\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m    281\u001b[0m ]\n\u001b[1;32m    282\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[1;32m    283\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[0;32m--> 284\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m    285\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m    286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m    287\u001b[0m         args,\n\u001b[1;32m    288\u001b[0m         kwargs,\n\u001b[1;32m    289\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    290\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m    291\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m    292\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m    293\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[1;32m    294\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m    295\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m    296\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    301\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1283\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1283\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m   1285\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:645\u001b[0m, in \u001b[0;36mFunction._compiler_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    642\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    643\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    644\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 645\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39;49m__wrapped__(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    646\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1258\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[39m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[39;00m\n\u001b[1;32m   1257\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1258\u001b[0m   \u001b[39mreturn\u001b[39;00m autograph\u001b[39m.\u001b[39;49mconverted_call(\n\u001b[1;32m   1259\u001b[0m       original_func,\n\u001b[1;32m   1260\u001b[0m       args,\n\u001b[1;32m   1261\u001b[0m       kwargs,\n\u001b[1;32m   1262\u001b[0m       options\u001b[39m=\u001b[39;49mautograph\u001b[39m.\u001b[39;49mConversionOptions(\n\u001b[1;32m   1263\u001b[0m           recursive\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1264\u001b[0m           optional_features\u001b[39m=\u001b[39;49mautograph_options,\n\u001b[1;32m   1265\u001b[0m           user_requested\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1266\u001b[0m       ))\n\u001b[1;32m   1267\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/var/folders/lf/1gtjn67j3gb56ps1bx1fyps00000gn/T/__autograph_generated_filevwt68fyq.py:17\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__f\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(_pfor_impl), (ag__\u001b[39m.\u001b[39;49mld(loop_fn), ag__\u001b[39m.\u001b[39;49mld(iters)), \u001b[39mdict\u001b[39;49m(fallback_to_while_loop\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(fallback_to_while_loop), parallel_iterations\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(parallel_iterations), warn\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(warn)), fscope)\n\u001b[1;32m     18\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39mif\u001b[39;00m conversion\u001b[39m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[1;32m    330\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: from cache\u001b[39m\u001b[39m'\u001b[39m, f)\n\u001b[0;32m--> 331\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mcontrol_status_ctx()\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m ag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mDISABLED:\n\u001b[1;32m    334\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: AutoGraph is disabled in context\u001b[39m\u001b[39m'\u001b[39m, f)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    457\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    459\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:340\u001b[0m, in \u001b[0;36m_pfor_impl\u001b[0;34m(loop_fn, iters, fallback_to_while_loop, parallel_iterations, pfor_config, warn)\u001b[0m\n\u001b[1;32m    338\u001b[0m     flattened_output_tensors \u001b[39m=\u001b[39m []\n\u001b[1;32m    339\u001b[0m     \u001b[39mfor\u001b[39;00m loop_fn_output \u001b[39min\u001b[39;00m nest\u001b[39m.\u001b[39mflatten(loop_fn_output_tensors):\n\u001b[0;32m--> 340\u001b[0m       output \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39;49mconvert(loop_fn_output)\n\u001b[1;32m    341\u001b[0m       flattened_output_tensors\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m    342\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/ops/parallel_for/pfor.py:1424\u001b[0m, in \u001b[0;36mPFor.convert\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_sparse(y)\n\u001b[1;32m   1423\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(y, (ops\u001b[39m.\u001b[39mTensor, ops\u001b[39m.\u001b[39mOperation)), y\n\u001b[0;32m-> 1424\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_helper(y)\n\u001b[1;32m   1425\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, WrappedTensor):\n\u001b[1;32m   1426\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(y, ops\u001b[39m.\u001b[39mTensor)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/ops/parallel_for/pfor.py:1653\u001b[0m, in \u001b[0;36mPFor._convert_helper\u001b[0;34m(self, op_or_tensor)\u001b[0m\n\u001b[1;32m   1650\u001b[0m     new_outputs \u001b[39m=\u001b[39m [new_outputs]\n\u001b[1;32m   1651\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(new_outputs,\n\u001b[1;32m   1652\u001b[0m                     (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, ops\u001b[39m.\u001b[39mOperation)), new_outputs\n\u001b[0;32m-> 1653\u001b[0m logging\u001b[39m.\u001b[39mvlog(\u001b[39m2\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconverted \u001b[39m\u001b[39m{\u001b[39;00my_op\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mnew_outputs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1655\u001b[0m \u001b[39m# Insert into self._conversion_map\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m y_op:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:2500\u001b[0m, in \u001b[0;36mOperation.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2499\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__str__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m-> 2500\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_def)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/google/protobuf/internal/python_message.py:1011\u001b[0m, in \u001b[0;36m_AddStrMethod.<locals>.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__str__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m-> 1011\u001b[0m   \u001b[39mreturn\u001b[39;00m text_format\u001b[39m.\u001b[39;49mMessageToString(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/google/protobuf/text_format.py:187\u001b[0m, in \u001b[0;36mMessageToString\u001b[0;34m(message, as_utf8, as_one_line, use_short_repeated_primitives, pointy_brackets, use_index_order, float_format, double_format, use_field_number, descriptor_pool, indent, message_formatter, print_unknown_fields, force_colon)\u001b[0m\n\u001b[1;32m    171\u001b[0m out \u001b[39m=\u001b[39m TextWriter(as_utf8)\n\u001b[1;32m    172\u001b[0m printer \u001b[39m=\u001b[39m _Printer(\n\u001b[1;32m    173\u001b[0m     out,\n\u001b[1;32m    174\u001b[0m     indent,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     print_unknown_fields\u001b[39m=\u001b[39mprint_unknown_fields,\n\u001b[1;32m    186\u001b[0m     force_colon\u001b[39m=\u001b[39mforce_colon)\n\u001b[0;32m--> 187\u001b[0m printer\u001b[39m.\u001b[39;49mPrintMessage(message)\n\u001b[1;32m    188\u001b[0m result \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    189\u001b[0m out\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/google/protobuf/text_format.py:444\u001b[0m, in \u001b[0;36m_Printer.PrintMessage\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(value):\n\u001b[1;32m    438\u001b[0m     \u001b[39m# This is slow for maps with submessage entries because it copies the\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     \u001b[39m# entire tree.  Unfortunately this would take significant refactoring\u001b[39;00m\n\u001b[1;32m    440\u001b[0m     \u001b[39m# of this file to work around.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     \u001b[39m# TODO(haberman): refactor and optimize if this becomes an issue.\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     entry_submsg \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mGetEntryClass()(key\u001b[39m=\u001b[39mkey, value\u001b[39m=\u001b[39mvalue[key])\n\u001b[0;32m--> 444\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mPrintField(field, entry_submsg)\n\u001b[1;32m    445\u001b[0m \u001b[39melif\u001b[39;00m field\u001b[39m.\u001b[39mlabel \u001b[39m==\u001b[39m descriptor\u001b[39m.\u001b[39mFieldDescriptor\u001b[39m.\u001b[39mLABEL_REPEATED:\n\u001b[1;32m    446\u001b[0m   \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_short_repeated_primitives\n\u001b[1;32m    447\u001b[0m       \u001b[39mand\u001b[39;00m field\u001b[39m.\u001b[39mcpp_type \u001b[39m!=\u001b[39m descriptor\u001b[39m.\u001b[39mFieldDescriptor\u001b[39m.\u001b[39mCPPTYPE_MESSAGE\n\u001b[1;32m    448\u001b[0m       \u001b[39mand\u001b[39;00m field\u001b[39m.\u001b[39mcpp_type \u001b[39m!=\u001b[39m descriptor\u001b[39m.\u001b[39mFieldDescriptor\u001b[39m.\u001b[39mCPPTYPE_STRING):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/google/protobuf/text_format.py:547\u001b[0m, in \u001b[0;36m_Printer.PrintField\u001b[0;34m(self, field, value)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_PrintFieldName(field)\n\u001b[1;32m    546\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 547\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mPrintFieldValue(field, value)\n\u001b[1;32m    548\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mas_one_line \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/google/protobuf/text_format.py:592\u001b[0m, in \u001b[0;36m_Printer.PrintFieldValue\u001b[0;34m(self, field, value)\u001b[0m\n\u001b[1;32m    590\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m field\u001b[39m.\u001b[39mcpp_type \u001b[39m==\u001b[39m descriptor\u001b[39m.\u001b[39mFieldDescriptor\u001b[39m.\u001b[39mCPPTYPE_MESSAGE:\n\u001b[0;32m--> 592\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_PrintMessageFieldValue(value)\n\u001b[1;32m    593\u001b[0m \u001b[39melif\u001b[39;00m field\u001b[39m.\u001b[39mcpp_type \u001b[39m==\u001b[39m descriptor\u001b[39m.\u001b[39mFieldDescriptor\u001b[39m.\u001b[39mCPPTYPE_ENUM:\n\u001b[1;32m    594\u001b[0m   enum_value \u001b[39m=\u001b[39m field\u001b[39m.\u001b[39menum_type\u001b[39m.\u001b[39mvalues_by_number\u001b[39m.\u001b[39mget(value, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/google/protobuf/text_format.py:577\u001b[0m, in \u001b[0;36m_Printer._PrintMessageFieldValue\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m openb)\n\u001b[1;32m    576\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m--> 577\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mPrintMessage(value)\n\u001b[1;32m    578\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    579\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent \u001b[39m+\u001b[39m closeb)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/google/protobuf/text_format.py:454\u001b[0m, in \u001b[0;36m_Printer.PrintMessage\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPrintField(field, element)\n\u001b[1;32m    453\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mPrintField(field, value)\n\u001b[1;32m    456\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_unknown_fields:\n\u001b[1;32m    457\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_PrintUnknownFields(message\u001b[39m.\u001b[39mUnknownFields())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/google/protobuf/text_format.py:547\u001b[0m, in \u001b[0;36m_Printer.PrintField\u001b[0;34m(self, field, value)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_PrintFieldName(field)\n\u001b[1;32m    546\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 547\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mPrintFieldValue(field, value)\n\u001b[1;32m    548\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mas_one_line \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/google/protobuf/text_format.py:592\u001b[0m, in \u001b[0;36m_Printer.PrintFieldValue\u001b[0;34m(self, field, value)\u001b[0m\n\u001b[1;32m    590\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m field\u001b[39m.\u001b[39mcpp_type \u001b[39m==\u001b[39m descriptor\u001b[39m.\u001b[39mFieldDescriptor\u001b[39m.\u001b[39mCPPTYPE_MESSAGE:\n\u001b[0;32m--> 592\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_PrintMessageFieldValue(value)\n\u001b[1;32m    593\u001b[0m \u001b[39melif\u001b[39;00m field\u001b[39m.\u001b[39mcpp_type \u001b[39m==\u001b[39m descriptor\u001b[39m.\u001b[39mFieldDescriptor\u001b[39m.\u001b[39mCPPTYPE_ENUM:\n\u001b[1;32m    594\u001b[0m   enum_value \u001b[39m=\u001b[39m field\u001b[39m.\u001b[39menum_type\u001b[39m.\u001b[39mvalues_by_number\u001b[39m.\u001b[39mget(value, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/google/protobuf/text_format.py:577\u001b[0m, in \u001b[0;36m_Printer._PrintMessageFieldValue\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m openb)\n\u001b[1;32m    576\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m--> 577\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mPrintMessage(value)\n\u001b[1;32m    578\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    579\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent \u001b[39m+\u001b[39m closeb)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/google/protobuf/text_format.py:454\u001b[0m, in \u001b[0;36m_Printer.PrintMessage\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPrintField(field, element)\n\u001b[1;32m    453\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mPrintField(field, value)\n\u001b[1;32m    456\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_unknown_fields:\n\u001b[1;32m    457\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_PrintUnknownFields(message\u001b[39m.\u001b[39mUnknownFields())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/google/protobuf/text_format.py:546\u001b[0m, in \u001b[0;36m_Printer.PrintField\u001b[0;34m(self, field, value)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[39m\"\"\"Print a single field name/value pair.\"\"\"\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_PrintFieldName(field)\n\u001b[0;32m--> 546\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout\u001b[39m.\u001b[39;49mwrite(\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    547\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPrintFieldValue(field, value)\n\u001b[1;32m    548\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mas_one_line \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dev/lib/python3.10/site-packages/google/protobuf/text_format.py:104\u001b[0m, in \u001b[0;36mTextWriter.write\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite\u001b[39m(\u001b[39mself\u001b[39m, val):\n\u001b[0;32m--> 104\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_writer\u001b[39m.\u001b[39;49mwrite(val)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_spectro_pipeline = mel_spectrogram_pipeline(data_preprocessing_pipeline._SET_OUTPUT_DIR)\n",
    "\n",
    "data_spectro_pipeline.clean_dirs()\n",
    "data_spectro_pipeline.get_preprocessed_files()\n",
    "\n",
    "print('\\nThis process will take approx 5-10 mins to complete depending on the power of your PC')\n",
    "data_spectro_pipeline.generate_mel_spectrograms(MEL_SPECTRO=True, SHOW_PLOT=False, FREQ_MASK=True, TIME_MASK=True, TORCH_EXPORT=False, TFIO_EXPORT=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline to load data into memory for model training\n",
    "\n",
    "This pipeline will load all the tensors into a train, test, and validation data structure and prepare it for inputs into a model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_test_vali_pipeline():\n",
    "    def __init__(self, output_directory) -> None:\n",
    "        self.DATASET_PATH  = output_directory\n",
    "        self.TENSOR_OUTPUT_DIR  = None\n",
    "\n",
    "        if '/' in self.DATASET_PATH:\n",
    "            self.TENSOR_OUTPUT_DIR = os.path.join(self.DATASET_PATH, 'OUTPUT_tensors')\n",
    "        elif '\\\\' in self.DATASET_PATH:\n",
    "            self.TENSOR_OUTPUT_DIR = os.path.join(self.DATASET_PATH, 'OUTPUT_tensors')\n",
    "\n",
    "        self.VALID_FILES = False\n",
    "\n",
    "        self.PATHS = []\n",
    "        self.train_data = self.test_data = self.vali_data = pd.DataFrame(columns=['Label', 'Tensor'])\n",
    "\n",
    "    \n",
    "    def check_valid_dirs(self) -> None:\n",
    "        def contains_test(arr):\n",
    "            if any(\"TEST\" in item for item in arr): return True\n",
    "            return False\n",
    "        def contains_train(arr):\n",
    "            if any(\"TRAIN\" in item for item in arr): return True\n",
    "            return False\n",
    "        def contains_vali(arr):\n",
    "            if any(\"VALIDATION\" in item for item in arr): return True\n",
    "            return False\n",
    "\n",
    "        print('Checking to find train, test and vali directories inside tensors folder...')\n",
    "        for root, dir, files in os.walk(self.TENSOR_OUTPUT_DIR):\n",
    "            for i in dir:\n",
    "                self.PATHS.append(os.path.join(root, i))\n",
    "            if contains_test(dir) and contains_train(dir) and contains_vali(dir):\n",
    "                self.VALID_FILES = True\n",
    "                print('PASS')\n",
    "            else:\n",
    "                raise ValueError('Cannot find folders from previous pipline which include train, test and validation directories')\n",
    "            break\n",
    "\n",
    "    def load_data(self, LOAD_RAW:bool = False, \n",
    "                        LOAD_FREQ:bool = False, \n",
    "                        LOAD_TIME:bool = False, \n",
    "                        TO_CSV:bool = False, \n",
    "                        TORCH_LOAD:bool = False, \n",
    "                        TFIO_LOAD:bool = False) -> None:\n",
    "\n",
    "        for path in self.PATHS:\n",
    "            if 'TEST' in str(path) or 'TRAIN' in str(path) or 'VALIDATION' in str(path):\n",
    "                for root, dir, file in os.walk(path):\n",
    "                    for tmp_label in dir:  \n",
    "                        for file in [f for f in listdir(os.path.join(root, tmp_label)) if isfile(join(os.path.join(root, tmp_label), f))]:\n",
    "                            if '_raw_mel' in str(file) and LOAD_RAW:\n",
    "                                if TORCH_LOAD:\n",
    "                                    tmp_data = torch.load(os.path.join(path, tmp_label, file))\n",
    "                                if TFIO_LOAD:\n",
    "                                    tmp_data = tf.io.parse_tensor(tf.io.read_file(os.path.join(path, tmp_label, file)), tf.float32)\n",
    "                                    \n",
    "                                new_row = pd.Series({\"Label\": tmp_label,\n",
    "                                                    \"Tensor\": tmp_data})\n",
    "                                if \"TRAIN\" in path:\n",
    "                                    self.train_data = pd.concat([self.train_data, new_row.to_frame().T], ignore_index=True)\n",
    "                                elif \"TEST\" in path:\n",
    "                                    self.test_data = pd.concat([self.test_data, new_row.to_frame().T], ignore_index=True)\n",
    "                                elif \"VALIDATION\" in path:\n",
    "                                    self.vali_data = pd.concat([self.vali_data, new_row.to_frame().T], ignore_index=True)\n",
    "                            elif '_freq_mask' in str(file) and LOAD_FREQ:\n",
    "                                if TORCH_LOAD:\n",
    "                                    tmp_data = torch.load(os.path.join(path, tmp_label, file))\n",
    "                                if TFIO_LOAD:\n",
    "                                    tmp_data = tf.io.parse_tensor(tf.io.read_file(os.path.join(path, tmp_label, file)), tf.float32)\n",
    "\n",
    "                                new_row = pd.Series({\"Label\": tmp_label,\n",
    "                                                    \"Tensor\": tmp_data})\n",
    "                                if \"TRAIN\" in path:\n",
    "                                    self.train_data = pd.concat([self.train_data, new_row.to_frame().T], ignore_index=True)\n",
    "                                elif \"TEST\" in path:\n",
    "                                    self.test_data = pd.concat([self.test_data, new_row.to_frame().T], ignore_index=True)\n",
    "                                elif \"VALIDATION\" in path:\n",
    "                                    self.vali_data = pd.concat([self.vali_data, new_row.to_frame().T], ignore_index=True)\n",
    "                            elif '_time_mask' in str(file) and LOAD_TIME:\n",
    "                                if TORCH_LOAD:\n",
    "                                    tmp_data = torch.load(os.path.join(path, tmp_label, file))\n",
    "                                if TFIO_LOAD:\n",
    "                                    tmp_data = tf.io.parse_tensor(tf.io.read_file(os.path.join(path, tmp_label, file)), tf.float32)\n",
    "                                    \n",
    "                                new_row = pd.Series({\"Label\": tmp_label,\n",
    "                                                    \"Tensor\": tmp_data})\n",
    "                                if \"TRAIN\" in path:\n",
    "                                    self.train_data = pd.concat([self.train_data, new_row.to_frame().T], ignore_index=True)\n",
    "                                elif \"TEST\" in path:\n",
    "                                    self.test_data = pd.concat([self.test_data, new_row.to_frame().T], ignore_index=True)\n",
    "                                elif \"VALIDATION\" in path:\n",
    "                                    self.vali_data = pd.concat([self.vali_data, new_row.to_frame().T], ignore_index=True)\n",
    "\n",
    "        if TO_CSV:\n",
    "            self.train_data.to_csv(os.path.join(self.DATASET_PATH, 'train_df.csv'))\n",
    "            self.test_data.to_csv(os.path.join(self.DATASET_PATH, 'test_df.csv'))\n",
    "            self.vali_data.to_csv(os.path.join(self.DATASET_PATH, 'vali_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_pipeline = train_test_vali_pipeline(data_preprocessing_pipeline._SET_OUTPUT_DIR)\n",
    "\n",
    "model_data_pipeline.check_valid_dirs()\n",
    "\n",
    "print('This process will take approx 1 minute')\n",
    "model_data_pipeline.load_data(LOAD_RAW=True, LOAD_FREQ=True, LOAD_TIME=True, TO_CSV=False, TORCH_LOAD=False, TFIO_LOAD=True)\n",
    "\n",
    "print(\"Finished\")\n",
    "print(f'Train Shape: {model_data_pipeline.train_data.shape}, Test Shape: {model_data_pipeline.test_data.shape}, Validation Shape: {model_data_pipeline.vali_data.shape}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in model_data_pipeline.train_data.iterrows():\n",
    "    print(row.Tensor)\n",
    "    input()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f4acbc442a267d9c039005cb6a4c551bfe6da3ea17529ec6bc5615c49cd80131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
