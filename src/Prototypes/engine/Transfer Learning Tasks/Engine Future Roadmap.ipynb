{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55203f1a",
   "metadata": {},
   "source": [
    "# The following are plans for changes to the currnet engine pipeline\n",
    "\n",
    "(formarly called the optmised engin pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0543eb",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1000bc",
   "metadata": {},
   "source": [
    "## Augmentations\n",
    "\n",
    "### There are several categories of augmentations that can be trialled. These are listed below. \n",
    "\n",
    "#### In addition to they type of augmentation, is the way in which i can be applied. Options include\n",
    "\n",
    "- Applying each augmentation to the audiofiles in the Species folder, to increase the dataset file size. This would ensure each sample receives each augmentation type. This approach could be used in the case of overlaying background audio to the files. \n",
    "\n",
    "- Audio file augmentations with the Audiomentations library would likely be more appropriate to have as a probability of the effect being applied, and a range in which it is applied selected from randomly. This could be applied separately, or as a combined augmentation of many"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d87e95",
   "metadata": {},
   "source": [
    "#### Augmentations to improve generalisibility:\n",
    "\n",
    "This includes overlaying background audio to the clips. This provides a distinctly different file to train with and helps the model learn what the features are. The audio overlay will include noises that will  be commonly heard in the forest environment in the Otways National Park. Static noise is included as there may be a background humm of the microphone and processor for which the model should be trained against. Other suggestions are also to be added. A method of applying this is to have a clean sample of a background audio file, and randomly select a segment of this to overlay for each file. This could be run for each each noise type separately, or using a combined approach, or both.\n",
    "\n",
    "- Rain noise\n",
    "- Wind noise\n",
    "- Water/stream noise\n",
    "- Static noise\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a4ec2a",
   "metadata": {},
   "source": [
    "### Augmentations to audio files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def5d32",
   "metadata": {},
   "source": [
    "### Audiomentations:\n",
    "\n",
    "Augmentating the raw audio will help to increase the training dataset by applying small but significant changes to the audiofile that result in a different Mel Spectogram. There are many to choose from, but the ones most relevant to try are included below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0dfeed",
   "metadata": {},
   "source": [
    "- TimeStretch\n",
    "- PitchShift\n",
    "- TimeMask\n",
    "- FrequencyMask\n",
    "- SevenBandParametricEQ\n",
    "- AddGausianSNR\n",
    "- AddBackgroundNoise\n",
    "- \n",
    "- (volume shift)\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f90ad",
   "metadata": {},
   "source": [
    "The parameters for which there are applied should be selectrd for within a range that is suitable for that genus.\n",
    "\n",
    "For example, PitchShift on a bird will likely not be suitable, as their calls have a distinctive pitch. Likewise for TimeStretch.\n",
    "\n",
    "However for other categories such as goats, cats, boar this could be a meaningful augmentation as vocalisaitons naturally lie within a range, depending on individual animals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b78d14",
   "metadata": {},
   "source": [
    "### Image augmentations:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59b0d1",
   "metadata": {},
   "source": [
    "After the audio file has been converted to a Mel Spectogram, the usual computer vision-based augmentations can be applied. However, once again there is only a subset for which are applicable for Mel Spectograms. Options to try include:\n",
    "\n",
    "- time masking with black, white, and mean colour masks\n",
    "- frequency masking with black, white, and mean colour masks\n",
    "- contrast adjustments\n",
    "-\n",
    "-\n",
    "-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da93f16",
   "metadata": {},
   "source": [
    "## Changes to the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e7ae0",
   "metadata": {},
   "source": [
    "### The model architecture itself may not be optimal for training on the current dataset. \n",
    "\n",
    "### The current limitations to the dataset include:\n",
    "\n",
    "#### Small number of audio files for some species\n",
    "\n",
    "Endangeres species tend to have few files available, but are priority spcecies to include, so pose a issue to be resolved. Possibile solutions include:\n",
    "- selectively applying many augmentations to those files to artificially increase dataset size\n",
    "- creating a separate model that is trained on under-represented species\n",
    "- expanding the search for more data\n",
    "\n",
    "#### Large number of audio files for some species\n",
    "\n",
    "There are many species for which there is an abundance of audio availaboe, and selectively decreasing the number of files used may be an option. Appling augmentations to species that are represented around the mean/median could be an option also, to match the number of files. There are commonly large number of files for pests, domestic animals, common species of birds, and frogs.\n",
    "\n",
    "#### Together leades to an imbalance in the model's training files\n",
    "\n",
    "---\n",
    "\n",
    "#### Raw full-length audio files that have non-animal vocalisations. \n",
    "\n",
    "The audio sourced is almost always not *clean* and without proper pre-processing and cleaning of the datasource, the model not learn correctly. The commonly found issues include:\n",
    "\n",
    "- Vocalisations of other species of that genus (eg. several species of birds in the one file)\n",
    "- Vocalisations of different genus (eg. frog, insect and bird noises in the one file)\n",
    "- Human voices are often present at the beginning of files, introducing that species.\n",
    "    \n",
    "####  Incorrectly classified segments\n",
    "\n",
    "The cleaning process currently is to use YamNet as event detection for classes for which that vocalisation falls under. The limitations to this approach include:\n",
    "\n",
    "- Limited set of audio event types that YamNet can classify\n",
    "- Misclassificaiton of audio events by YamNet\n",
    "\n",
    "Together this can lead to a dataset of segments of audio for which many are not vocalisations of that species. \n",
    "\n",
    "Other parameters include the length of audio extracted when an event is detected.\n",
    "\n",
    "#### Current solutions being investifgated\n",
    "\n",
    "Fine tuning of the parameters used in YamNet is the first approach to take. \n",
    "A ML approach of cluster analysis is also being trialed to group events into clusters. The number of clusters can be tweaked.\n",
    "Manual cleaning of the audio files is the immediate solution to clean the data. \n",
    "Further, combining these approaches is running clustering on a speceis, and grouping clusters for which include positive audio of that species.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221c41a",
   "metadata": {},
   "source": [
    "## Changes to the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1828f0a",
   "metadata": {},
   "source": [
    "### The model architecture can also be investigated. This involves:\n",
    "\n",
    "#### Using different imported (transfer-learning) models\n",
    "\n",
    "There are many computer-vision models that are pre-trained on images. The current enging pipeline use ***ImageNet*** however the are many others to trial:\n",
    "\n",
    "- VGG16\n",
    "- Inception\n",
    "- Xception\n",
    "- EfficientNet\n",
    "- \n",
    "-\n",
    "- \n",
    "\n",
    "#### Using different imported models trained for audio classification. \n",
    "\n",
    "These are models that have been trained on Mel Spectograms and may provide more relevant feature embeddings for audio-based machine learning. Model to try include:\n",
    "\n",
    "- Bird Vocalisation Classifier\n",
    "- SoundNet\n",
    "- VGGish\n",
    "\n",
    "### Modifying the layers used after the imported model.\n",
    "\n",
    "To prevent overfitting, it may be necessary to make changes to the current architecture of the model. This could include more dropout layers, or differnt kinds of condensing the network layers and using different activation functions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
