{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew\\AppData\\Local\\Temp\\ipykernel_21308\\922995980.py:14: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version     :  3.9.13\n"
     ]
    }
   ],
   "source": [
    "# some basic libraries\n",
    "import sys  \n",
    "import os\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import average\n",
    "import math\n",
    "import bisect\n",
    "import pickle\n",
    "import random\n",
    "from platform import python_version\n",
    "\n",
    "# ipython display\n",
    "from IPython.core.display import display\n",
    "\n",
    "# pytorch lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import transforms\n",
    "from torchlibrosa.stft import STFT, ISTFT, magphase\n",
    "\n",
    "# librosa audio processing\n",
    "import librosa\n",
    "\n",
    "# sound file\n",
    "import soundfile as sf\n",
    "\n",
    "# sk learn machine learning library\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score\n",
    "\n",
    "# tensorboard for monitoring training progress\n",
    "import tensorboard\n",
    "\n",
    "from htsat_utils import do_mixup, get_mix_lambda, do_mixup_label, get_loss_func, d_prime\n",
    "\n",
    "import htsat_config\n",
    "from htsat_model import HTSAT_Swin_Transformer \n",
    "\n",
    "# print system information\n",
    "print('Python Version     : ', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - data pipeline in pytorch lightening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrew\\miniconda3\\envs\\dev\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the HTSAT model\n",
    "model = HTSAT_Swin_Transformer(\n",
    "    spec_size=htsat_config.htsat_spec_size,\n",
    "    patch_size=htsat_config.htsat_patch_size,\n",
    "    in_chans=1,\n",
    "    num_classes=htsat_config.classes_num,\n",
    "    window_size=htsat_config.htsat_window_size,\n",
    "    config = htsat_config,\n",
    "    depths = htsat_config.htsat_depth,\n",
    "    embed_dim = htsat_config.htsat_dim,\n",
    "    patch_stride=htsat_config.htsat_stride,\n",
    "    num_heads=htsat_config.htsat_num_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HTSAT_Swin_Transformer(\n",
       "  (spectrogram_extractor): Spectrogram(\n",
       "    (stft): STFT(\n",
       "      (conv_real): Conv1d(1, 513, kernel_size=(1024,), stride=(128,), bias=False)\n",
       "      (conv_imag): Conv1d(1, 513, kernel_size=(1024,), stride=(128,), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (logmel_extractor): LogmelFilterBank()\n",
       "  (spec_augmenter): SpecAugmentation(\n",
       "    (time_dropper): DropStripes()\n",
       "    (freq_dropper): DropStripes()\n",
       "  )\n",
       "  (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): BasicLayer(\n",
       "      dim=96, input_resolution=(64, 64), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(64, 64), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=96, window_size=(8, 8), num_heads=4\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(64, 64), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=96, window_size=(8, 8), num_heads=4\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(64, 64), dim=96\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      dim=192, input_resolution=(32, 32), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(32, 32), num_heads=8, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=192, window_size=(8, 8), num_heads=8\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(32, 32), num_heads=8, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=192, window_size=(8, 8), num_heads=8\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(32, 32), dim=192\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      dim=384, input_resolution=(16, 16), depth=6\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(8, 8), num_heads=16\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(8, 8), num_heads=16\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(8, 8), num_heads=16\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(8, 8), num_heads=16\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(8, 8), num_heads=16\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(8, 8), num_heads=16\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(16, 16), dim=384\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      dim=768, input_resolution=(8, 8), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(8, 8), num_heads=32, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(8, 8), num_heads=32\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(8, 8), num_heads=32, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(8, 8), num_heads=32\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (maxpool): AdaptiveMaxPool1d(output_size=1)\n",
       "  (tscam_conv): Conv2d(768, 5, kernel_size=(4, 3), stride=(1, 1), padding=(0, 1))\n",
       "  (head): Linear(in_features=5, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show model details\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEDWrapper(pl.LightningModule):\n",
    "    def __init__(self, sed_model, config, dataset):\n",
    "        super().__init__()\n",
    "        self.sed_model = sed_model\n",
    "        self.config = config\n",
    "        self.dataset = dataset\n",
    "        self.loss_func = get_loss_func(config.loss_type)\n",
    "\n",
    "    def evaluate_metric(self, pred, ans):\n",
    "        ap = []\n",
    "        if self.config.dataset_type == \"audioset\":\n",
    "            mAP = np.mean(average_precision_score(ans, pred, average = None))\n",
    "            mAUC = np.mean(roc_auc_score(ans, pred, average = None))\n",
    "            dprime = d_prime(mAUC)\n",
    "            return {\"mAP\": mAP, \"mAUC\": mAUC, \"dprime\": dprime}\n",
    "        else:\n",
    "            acc = accuracy_score(ans, np.argmax(pred, 1))\n",
    "            return {\"acc\": acc}  \n",
    "    def forward(self, x, mix_lambda = None):\n",
    "        output_dict = self.sed_model(x, mix_lambda)\n",
    "        return output_dict[\"clipwise_output\"], output_dict[\"framewise_output\"]\n",
    "\n",
    "    def inference(self, x):\n",
    "        self.device_type = next(self.parameters()).device\n",
    "        self.eval()\n",
    "        x = torch.from_numpy(x).float().to(self.device_type)\n",
    "        output_dict = self.sed_model(x, None, True)\n",
    "        for key in output_dict.keys():\n",
    "            output_dict[key] = output_dict[key].detach().cpu().numpy()\n",
    "        return output_dict\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.device_type = next(self.parameters()).device\n",
    "        if self.config.dataset_type == \"audioset\":\n",
    "            mix_lambda = torch.from_numpy(get_mix_lambda(0.5, len(batch[\"waveform\"]))).to(self.device_type)\n",
    "        else:\n",
    "            mix_lambda = None\n",
    "\n",
    "        # Another Choice: also mixup the target, but AudioSet is not a perfect data\n",
    "        # so \"adding noise\" might be better than purly \"mix\"\n",
    "        # batch[\"target\"] = do_mixup_label(batch[\"target\"])\n",
    "        # batch[\"target\"] = do_mixup(batch[\"target\"], mix_lambda)\n",
    "        \n",
    "        pred, _ = self(batch[\"waveform\"], mix_lambda)\n",
    "        loss = self.loss_func(pred, batch[\"target\"])\n",
    "        self.log(\"loss\", loss, on_epoch= True, prog_bar=True)\n",
    "        return loss\n",
    "        \n",
    "    def training_epoch_end(self, outputs):\n",
    "        # Change: SWA, deprecated\n",
    "        # for opt in self.trainer.optimizers:\n",
    "        #     if not type(opt) is SWA:\n",
    "        #         continue\n",
    "        #     opt.swap_swa_sgd()\n",
    "        self.dataset.generate_queue()\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pred, _ = self(batch[\"waveform\"])\n",
    "        return [pred.detach(), batch[\"target\"].detach()]\n",
    "    \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        self.device_type = next(self.parameters()).device\n",
    "        pred = torch.cat([d[0] for d in validation_step_outputs], dim = 0)\n",
    "        target = torch.cat([d[1] for d in validation_step_outputs], dim = 0)\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            gather_pred = [torch.zeros_like(pred) for _ in range(dist.get_world_size())]\n",
    "            gather_target = [torch.zeros_like(target) for _ in range(dist.get_world_size())]\n",
    "            dist.barrier()\n",
    "\n",
    "        if self.config.dataset_type == \"audioset\":\n",
    "            metric_dict = {\n",
    "                \"mAP\": 0.,\n",
    "                \"mAUC\": 0.,\n",
    "                \"dprime\": 0.\n",
    "            }\n",
    "        else:\n",
    "            metric_dict = {\n",
    "                \"acc\":0.\n",
    "            }\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            dist.all_gather(gather_pred, pred)\n",
    "            dist.all_gather(gather_target, target)\n",
    "            if dist.get_rank() == 0:\n",
    "                gather_pred = torch.cat(gather_pred, dim = 0).cpu().numpy()\n",
    "                gather_target = torch.cat(gather_target, dim = 0).cpu().numpy()\n",
    "                if self.config.dataset_type == \"scv2\":\n",
    "                    gather_target = np.argmax(gather_target, 1)\n",
    "                metric_dict = self.evaluate_metric(gather_pred, gather_target)\n",
    "                print(self.device_type, dist.get_world_size(), metric_dict, flush = True)\n",
    "        \n",
    "            if self.config.dataset_type == \"audioset\":\n",
    "                self.log(\"mAP\", metric_dict[\"mAP\"] * float(dist.get_world_size()), on_epoch = True, prog_bar=True, sync_dist=True)\n",
    "                self.log(\"mAUC\", metric_dict[\"mAUC\"] * float(dist.get_world_size()), on_epoch = True, prog_bar=True, sync_dist=True)\n",
    "                self.log(\"dprime\", metric_dict[\"dprime\"] * float(dist.get_world_size()), on_epoch = True, prog_bar=True, sync_dist=True)\n",
    "            else:\n",
    "                self.log(\"acc\", metric_dict[\"acc\"] * float(dist.get_world_size()), on_epoch = True, prog_bar=True, sync_dist=True)\n",
    "            dist.barrier()\n",
    "        else:\n",
    "            gather_pred = pred.cpu().numpy()\n",
    "            gather_target = target.cpu().numpy()\n",
    "            if self.config.dataset_type == \"scv2\":\n",
    "                gather_target = np.argmax(gather_target, 1)\n",
    "            metric_dict = self.evaluate_metric(gather_pred, gather_target)\n",
    "            print(self.device_type, metric_dict, flush = True)\n",
    "        \n",
    "            if self.config.dataset_type == \"audioset\":\n",
    "                self.log(\"mAP\", metric_dict[\"mAP\"], on_epoch = True, prog_bar=True, sync_dist=False)\n",
    "                self.log(\"mAUC\", metric_dict[\"mAUC\"], on_epoch = True, prog_bar=True, sync_dist=False)\n",
    "                self.log(\"dprime\", metric_dict[\"dprime\"], on_epoch = True, prog_bar=True, sync_dist=False)\n",
    "            else:\n",
    "                self.log(\"acc\", metric_dict[\"acc\"], on_epoch = True, prog_bar=True, sync_dist=False)\n",
    "            \n",
    "        \n",
    "    def time_shifting(self, x, shift_len):\n",
    "        shift_len = int(shift_len)\n",
    "        new_sample = torch.cat([x[:, shift_len:], x[:, :shift_len]], axis = 1)\n",
    "        return new_sample \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.device_type = next(self.parameters()).device\n",
    "        preds = []\n",
    "        # time shifting optimization\n",
    "        if self.config.fl_local or self.config.dataset_type != \"audioset\": \n",
    "            shift_num = 1 # framewise localization cannot allow the time shifting\n",
    "        else:\n",
    "            shift_num = 10 \n",
    "        for i in range(shift_num):\n",
    "            pred, pred_map = self(batch[\"waveform\"])\n",
    "            preds.append(pred.unsqueeze(0))\n",
    "            batch[\"waveform\"] = self.time_shifting(batch[\"waveform\"], shift_len = 100 * (i + 1))\n",
    "        preds = torch.cat(preds, dim=0)\n",
    "        pred = preds.mean(dim = 0)\n",
    "        if self.config.fl_local:\n",
    "            return [\n",
    "                pred.detach().cpu().numpy(), \n",
    "                pred_map.detach().cpu().numpy(),\n",
    "                batch[\"audio_name\"],\n",
    "                batch[\"real_len\"].cpu().numpy()\n",
    "            ]\n",
    "        else:\n",
    "            return [pred.detach(), batch[\"target\"].detach()]\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        self.device_type = next(self.parameters()).device\n",
    "        if self.config.fl_local:\n",
    "            pred = np.concatenate([d[0] for d in test_step_outputs], axis = 0)\n",
    "            pred_map = np.concatenate([d[1] for d in test_step_outputs], axis = 0)\n",
    "            audio_name = np.concatenate([d[2] for d in test_step_outputs], axis = 0)\n",
    "            real_len = np.concatenate([d[3] for d in test_step_outputs], axis = 0)\n",
    "            heatmap_file = os.path.join(self.config.heatmap_dir, self.config.test_file + \"_\" + str(self.device_type) + \".npy\")\n",
    "            save_npy = [\n",
    "                {\n",
    "                    \"audio_name\": audio_name[i],\n",
    "                    \"heatmap\": pred_map[i],\n",
    "                    \"pred\": pred[i],\n",
    "                    \"real_len\":real_len[i]\n",
    "                }\n",
    "                for i in range(len(pred))\n",
    "            ]\n",
    "            np.save(heatmap_file, save_npy)\n",
    "        else:\n",
    "            self.device_type = next(self.parameters()).device\n",
    "            pred = torch.cat([d[0] for d in test_step_outputs], dim = 0)\n",
    "            target = torch.cat([d[1] for d in test_step_outputs], dim = 0)\n",
    "            gather_pred = [torch.zeros_like(pred) for _ in range(dist.get_world_size())]\n",
    "            gather_target = [torch.zeros_like(target) for _ in range(dist.get_world_size())]\n",
    "            dist.barrier()\n",
    "            if self.config.dataset_type == \"audioset\":\n",
    "                metric_dict = {\n",
    "                \"mAP\": 0.,\n",
    "                \"mAUC\": 0.,\n",
    "                \"dprime\": 0.\n",
    "                }\n",
    "            else:\n",
    "                metric_dict = {\n",
    "                    \"acc\":0.\n",
    "                }\n",
    "            dist.all_gather(gather_pred, pred)\n",
    "            dist.all_gather(gather_target, target)\n",
    "            if dist.get_rank() == 0:\n",
    "                gather_pred = torch.cat(gather_pred, dim = 0).cpu().numpy()\n",
    "                gather_target = torch.cat(gather_target, dim = 0).cpu().numpy()\n",
    "                if self.config.dataset_type == \"scv2\":\n",
    "                    gather_target = np.argmax(gather_target, 1)\n",
    "                metric_dict = self.evaluate_metric(gather_pred, gather_target)\n",
    "                print(self.device_type, dist.get_world_size(), metric_dict, flush = True)\n",
    "            if self.config.dataset_type == \"audioset\":\n",
    "                self.log(\"mAP\", metric_dict[\"mAP\"] * float(dist.get_world_size()), on_epoch = True, prog_bar=True, sync_dist=True)\n",
    "                self.log(\"mAUC\", metric_dict[\"mAUC\"] * float(dist.get_world_size()), on_epoch = True, prog_bar=True, sync_dist=True)\n",
    "                self.log(\"dprime\", metric_dict[\"dprime\"] * float(dist.get_world_size()), on_epoch = True, prog_bar=True, sync_dist=True)\n",
    "            else:\n",
    "                self.log(\"acc\", metric_dict[\"acc\"] * float(dist.get_world_size()), on_epoch = True, prog_bar=True, sync_dist=True)\n",
    "            dist.barrier()\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.config.learning_rate, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        # Change: SWA, deprecated\n",
    "        # optimizer = SWA(optimizer, swa_start=10, swa_freq=5)\n",
    "        def lr_foo(epoch):       \n",
    "            if epoch < 3:\n",
    "                # warm up lr\n",
    "                lr_scale = self.config.lr_rate[epoch]\n",
    "            else:\n",
    "                # warmup schedule\n",
    "                lr_pos = int(-1 - bisect.bisect_left(self.config.lr_scheduler_epoch, epoch))\n",
    "                if lr_pos < -3:\n",
    "                    lr_scale = max(self.config.lr_rate[0] * (0.98 ** epoch), 0.03 )\n",
    "                else:\n",
    "                    lr_scale = self.config.lr_rate[lr_pos]\n",
    "            return lr_scale\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda=lr_foo\n",
    "        )\n",
    "        \n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1660670939.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [6], line 18\u001b[1;36m\u001b[0m\n\u001b[1;33m    def configure_optimizers(self):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class EchoEngineModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__(model)\n",
    "        self.model = model\n",
    "        #self.l1 = torch.nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n",
    "        # return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        loss = F.cross_entropy(self(x), y)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "class data_prep(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, eval_dataset, device_num):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.device_num = device_num\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_sampler = DistributedSampler(self.train_dataset, shuffle = False) if self.device_num > 1 else None\n",
    "        train_loader = DataLoader(\n",
    "            dataset = self.train_dataset,\n",
    "            num_workers = config.num_workers,\n",
    "            batch_size = config.batch_size // self.device_num,\n",
    "            shuffle = False,\n",
    "            sampler = train_sampler\n",
    "        )\n",
    "        return train_loader\n",
    "    def val_dataloader(self):\n",
    "        eval_sampler = DistributedSampler(self.eval_dataset, shuffle = False) if self.device_num > 1 else None\n",
    "        eval_loader = DataLoader(\n",
    "            dataset = self.eval_dataset,\n",
    "            num_workers = config.num_workers,\n",
    "            batch_size = config.batch_size // self.device_num,\n",
    "            shuffle = False,\n",
    "            sampler = eval_sampler\n",
    "        )\n",
    "        return eval_loader\n",
    "    def test_dataloader(self):\n",
    "        test_sampler = DistributedSampler(self.eval_dataset, shuffle = False) if self.device_num > 1 else None\n",
    "        test_loader = DataLoader(\n",
    "            dataset = self.eval_dataset,\n",
    "            num_workers = config.num_workers,\n",
    "            batch_size = config.batch_size // self.device_num,\n",
    "            shuffle = False,\n",
    "            sampler = test_sampler\n",
    "        )\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_num = torch.cuda.device_count()\n",
    "\n",
    "audioset_data = data_prep(dataset, eval_dataset, device_num)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        deterministic=False,\n",
    "        default_root_dir = workspacecheckpoint_dir,\n",
    "        gpus = device_num, \n",
    "        val_check_interval = 0.1,\n",
    "        max_epochs = config.max_epoch,\n",
    "        auto_lr_find = True,    \n",
    "        sync_batchnorm = True,\n",
    "        #callbacks = [checkpoint_callback],\n",
    "        accelerator = \"ddp\" if device_num > 1 else None,\n",
    "        num_sanity_val_steps = 0,\n",
    "        resume_from_checkpoint = None, \n",
    "        replace_sampler_ddp = False,\n",
    "        gradient_clip_val=1.0\n",
    "    )\n",
    " \n",
    "sed_model = HTSAT_Swin_Transformer(\n",
    "        spec_size=config.htsat_spec_size,\n",
    "        patch_size=config.htsat_patch_size,\n",
    "        in_chans=1,\n",
    "        num_classes=config.classes_num,\n",
    "        window_size=config.htsat_window_size,\n",
    "        config = config,\n",
    "        depths = config.htsat_depth,\n",
    "        embed_dim = config.htsat_dim,\n",
    "        patch_stride=config.htsat_stride,\n",
    "        num_heads=config.htsat_num_head\n",
    "    )\n",
    " \n",
    "model = SEDWrapper(\n",
    "        sed_model = sed_model, \n",
    "        config = config,\n",
    "        dataset = dataset\n",
    "    )\n",
    " \n",
    "trainer.fit(model, audioset_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2848388c1d7df64c5912f8c74b12cb2f63a5fbb869f66edadd9f1eda580b6df3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
