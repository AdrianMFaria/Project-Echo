{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# basic imports\n",
    "import tensorflow as tf \n",
    "import torch\n",
    "import io\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.utils import dataset_utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import baseline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12384 files belonging to 5 classes.\n",
      "Found 487 files belonging to 5 classes.\n",
      "Found 384 files belonging to 5 classes.\n",
      "class names:  ['brant', 'jabwar', 'sheowl', 'spodov', 'wiltur']\n"
     ]
    }
   ],
   "source": [
    "def paths_and_labels_to_dataset(image_paths,labels,num_classes):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    img_ds = path_ds.map(\n",
    "        lambda path: tf.io.read_file(path), \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    label_ds = dataset_utils.labels_to_dataset(\n",
    "        labels, \n",
    "        'categorical', \n",
    "        num_classes)\n",
    "    img_ds = tf.data.Dataset.zip((img_ds, label_ds))\n",
    "    return img_ds\n",
    "\n",
    "def create_dataset(subset):\n",
    "    image_paths, labels, class_names = dataset_utils.index_directory(\n",
    "            baseline_config.dataset_path + subset,\n",
    "            labels=\"inferred\",\n",
    "            formats=('.pt'),\n",
    "            class_names=None,\n",
    "            shuffle=False,\n",
    "            seed=42,\n",
    "            follow_links=False)\n",
    "\n",
    "    dataset = paths_and_labels_to_dataset(\n",
    "        image_paths=image_paths,\n",
    "        labels=labels,\n",
    "        num_classes=len(class_names))\n",
    "    \n",
    "    return dataset, class_names\n",
    "\n",
    "train_dataset, class_names = create_dataset('TRAIN/')\n",
    "test_dataset, _            = create_dataset('TEST/')\n",
    "validation_dataset, _      = create_dataset('VALIDATION/')\n",
    "print(\"class names: \", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_transforms(image,label):\n",
    "    image = tf.io.parse_tensor(image, tf.float32)\n",
    "    image = tf.expand_dims(image, -1)\n",
    "    image = tf.repeat(image, 3, 2)\n",
    "    return image,label\n",
    "\n",
    "train_dataset_b = ( \n",
    "                  train_dataset\n",
    "                  .shuffle(20000)\n",
    "                  .map(dataset_transforms)\n",
    "                  .batch(baseline_config.batch_size)\n",
    "                  .cache()\n",
    "                  .repeat()            \n",
    "                )\n",
    "\n",
    "validation_dataset_b = ( \n",
    "                  validation_dataset\n",
    "                  .map(dataset_transforms)\n",
    "                  .batch(baseline_config.batch_size)\n",
    "                  .cache()\n",
    "                )\n",
    "\n",
    "test_dataset_b = ( \n",
    "                  test_dataset\n",
    "                  .map(dataset_transforms)\n",
    "                  .batch(baseline_config.batch_size)\n",
    "                  .cache()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 313, 128, 3) (8, 5)\n"
     ]
    }
   ],
   "source": [
    "for item,lbl in train_dataset_b.take(1):\n",
    "    print(item.shape, lbl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resizing_1 (Resizing)       (None, 480, 480, 3)       0         \n",
      "                                                                 \n",
      " keras_layer_1 (KerasLayer)  (None, 1280)              53150388  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                40992     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,191,545\n",
      "Trainable params: 52,899,513\n",
      "Non-trainable params: 292,032\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build a really simple classification model using a pre-training Efficientnet V2\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        # use the model as a feature generator only\n",
    "        # need to resize here, as the efficientnet_v2_imagenet21k_s model expects it\n",
    "        tf.keras.layers.InputLayer(input_shape=(313,128,3)),\n",
    "        tf.keras.layers.Resizing(480, 480, interpolation=\"lanczos5\", crop_to_aspect_ratio=False),\n",
    "        \n",
    "        # downloaded from: https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\n",
    "        hub.KerasLayer(\"imagenet_efficientnet_v2_imagenet21k_m_feature_vector_2\", True),        \n",
    "\n",
    "        # add the classification layer here       \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dropout(0.60),\n",
    "        layers.Dense(len(class_names), activation=None),\n",
    "    ]\n",
    ")\n",
    "# need to tell the model what the input shape is\n",
    "model.build([None, 313, 128, 3])\n",
    "\n",
    "# show the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100000\n",
      "target shape (None, 5)\n",
      "output shape (None, 5)\n",
      "target shape (None, 5)\n",
      "output shape (None, 5)\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.9223 - accuracy: 0.2390target shape (None, 5)\n",
      "output shape (None, 5)\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.44631, saving model to checkpoints\\\n",
      "250/250 [==============================] - 139s 450ms/step - loss: 1.9223 - accuracy: 0.2390 - val_loss: 1.4463 - val_accuracy: 0.4010\n",
      "Epoch 2/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.5865 - accuracy: 0.3110\n",
      "Epoch 2: val_loss improved from 1.44631 to 1.33470, saving model to checkpoints\\\n",
      "250/250 [==============================] - 150s 599ms/step - loss: 1.5865 - accuracy: 0.3110 - val_loss: 1.3347 - val_accuracy: 0.5521\n",
      "Epoch 3/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.4894 - accuracy: 0.3640\n",
      "Epoch 3: val_loss improved from 1.33470 to 1.27570, saving model to checkpoints\\\n",
      "250/250 [==============================] - 172s 685ms/step - loss: 1.4894 - accuracy: 0.3640 - val_loss: 1.2757 - val_accuracy: 0.5547\n",
      "Epoch 4/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.4345 - accuracy: 0.3835\n",
      "Epoch 4: val_loss improved from 1.27570 to 1.11624, saving model to checkpoints\\\n",
      "250/250 [==============================] - 166s 663ms/step - loss: 1.4345 - accuracy: 0.3835 - val_loss: 1.1162 - val_accuracy: 0.6302\n",
      "Epoch 5/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.3493 - accuracy: 0.4250\n",
      "Epoch 5: val_loss improved from 1.11624 to 1.09242, saving model to checkpoints\\\n",
      "250/250 [==============================] - 167s 668ms/step - loss: 1.3493 - accuracy: 0.4250 - val_loss: 1.0924 - val_accuracy: 0.6198\n",
      "Epoch 6/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.3000 - accuracy: 0.4730\n",
      "Epoch 6: val_loss improved from 1.09242 to 0.99904, saving model to checkpoints\\\n",
      "250/250 [==============================] - 153s 613ms/step - loss: 1.3000 - accuracy: 0.4730 - val_loss: 0.9990 - val_accuracy: 0.6693\n",
      "Epoch 7/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.2121 - accuracy: 0.5100\n",
      "Epoch 7: val_loss improved from 0.99904 to 0.91800, saving model to checkpoints\\\n",
      "250/250 [==============================] - 164s 656ms/step - loss: 1.2121 - accuracy: 0.5100 - val_loss: 0.9180 - val_accuracy: 0.6667\n",
      "Epoch 8/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.1556 - accuracy: 0.5470\n",
      "Epoch 8: val_loss improved from 0.91800 to 0.87128, saving model to checkpoints\\\n",
      "250/250 [==============================] - 156s 621ms/step - loss: 1.1556 - accuracy: 0.5470 - val_loss: 0.8713 - val_accuracy: 0.6849\n",
      "Epoch 9/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.0719 - accuracy: 0.5790\n",
      "Epoch 9: val_loss improved from 0.87128 to 0.76360, saving model to checkpoints\\\n",
      "250/250 [==============================] - 159s 633ms/step - loss: 1.0719 - accuracy: 0.5790 - val_loss: 0.7636 - val_accuracy: 0.7500\n",
      "Epoch 10/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.0277 - accuracy: 0.6025\n",
      "Epoch 10: val_loss improved from 0.76360 to 0.65673, saving model to checkpoints\\\n",
      "250/250 [==============================] - 163s 652ms/step - loss: 1.0277 - accuracy: 0.6025 - val_loss: 0.6567 - val_accuracy: 0.8099\n",
      "Epoch 11/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.0355 - accuracy: 0.5970\n",
      "Epoch 11: val_loss improved from 0.65673 to 0.63839, saving model to checkpoints\\\n",
      "250/250 [==============================] - 160s 641ms/step - loss: 1.0355 - accuracy: 0.5970 - val_loss: 0.6384 - val_accuracy: 0.7969\n",
      "Epoch 12/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.9695 - accuracy: 0.6355\n",
      "Epoch 12: val_loss did not improve from 0.63839\n",
      "250/250 [==============================] - 161s 644ms/step - loss: 0.9695 - accuracy: 0.6355 - val_loss: 0.6681 - val_accuracy: 0.7734\n",
      "Epoch 13/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.9106 - accuracy: 0.6615\n",
      "Epoch 13: val_loss improved from 0.63839 to 0.60744, saving model to checkpoints\\\n",
      "250/250 [==============================] - 155s 620ms/step - loss: 0.9106 - accuracy: 0.6615 - val_loss: 0.6074 - val_accuracy: 0.7865\n",
      "Epoch 14/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.8653 - accuracy: 0.6790\n",
      "Epoch 14: val_loss improved from 0.60744 to 0.58464, saving model to checkpoints\\\n",
      "250/250 [==============================] - 161s 642ms/step - loss: 0.8653 - accuracy: 0.6790 - val_loss: 0.5846 - val_accuracy: 0.8047\n",
      "Epoch 15/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.8066 - accuracy: 0.6980\n",
      "Epoch 15: val_loss improved from 0.58464 to 0.56083, saving model to checkpoints\\\n",
      "250/250 [==============================] - 166s 664ms/step - loss: 0.8066 - accuracy: 0.6980 - val_loss: 0.5608 - val_accuracy: 0.8151\n",
      "Epoch 16/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.8223 - accuracy: 0.6890\n",
      "Epoch 16: val_loss did not improve from 0.56083\n",
      "250/250 [==============================] - 162s 646ms/step - loss: 0.8223 - accuracy: 0.6890 - val_loss: 0.6200 - val_accuracy: 0.7760\n",
      "Epoch 17/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.7507 - accuracy: 0.7155\n",
      "Epoch 17: val_loss improved from 0.56083 to 0.51453, saving model to checkpoints\\\n",
      "250/250 [==============================] - 166s 665ms/step - loss: 0.7507 - accuracy: 0.7155 - val_loss: 0.5145 - val_accuracy: 0.8203\n",
      "Epoch 18/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.7190 - accuracy: 0.7380\n",
      "Epoch 18: val_loss did not improve from 0.51453\n",
      "250/250 [==============================] - 162s 646ms/step - loss: 0.7190 - accuracy: 0.7380 - val_loss: 0.5891 - val_accuracy: 0.7839\n",
      "Epoch 19/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.7250 - accuracy: 0.7410\n",
      "Epoch 19: val_loss did not improve from 0.51453\n",
      "250/250 [==============================] - 158s 631ms/step - loss: 0.7250 - accuracy: 0.7410 - val_loss: 0.5771 - val_accuracy: 0.7995\n",
      "Epoch 20/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6696 - accuracy: 0.7640\n",
      "Epoch 20: val_loss improved from 0.51453 to 0.50409, saving model to checkpoints\\\n",
      "250/250 [==============================] - 166s 665ms/step - loss: 0.6696 - accuracy: 0.7640 - val_loss: 0.5041 - val_accuracy: 0.8203\n",
      "Epoch 21/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6512 - accuracy: 0.7600\n",
      "Epoch 21: val_loss improved from 0.50409 to 0.49782, saving model to checkpoints\\\n",
      "250/250 [==============================] - 167s 667ms/step - loss: 0.6512 - accuracy: 0.7600 - val_loss: 0.4978 - val_accuracy: 0.8359\n",
      "Epoch 22/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6135 - accuracy: 0.7825\n",
      "Epoch 22: val_loss improved from 0.49782 to 0.47786, saving model to checkpoints\\\n",
      "250/250 [==============================] - 166s 664ms/step - loss: 0.6135 - accuracy: 0.7825 - val_loss: 0.4779 - val_accuracy: 0.8385\n",
      "Epoch 23/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5503 - accuracy: 0.8115\n",
      "Epoch 23: val_loss did not improve from 0.47786\n",
      "250/250 [==============================] - 162s 646ms/step - loss: 0.5503 - accuracy: 0.8115 - val_loss: 0.4818 - val_accuracy: 0.8333\n",
      "Epoch 24/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5686 - accuracy: 0.7935\n",
      "Epoch 24: val_loss improved from 0.47786 to 0.41323, saving model to checkpoints\\\n",
      "250/250 [==============================] - 159s 638ms/step - loss: 0.5686 - accuracy: 0.7935 - val_loss: 0.4132 - val_accuracy: 0.8568\n",
      "Epoch 25/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5722 - accuracy: 0.8095\n",
      "Epoch 25: val_loss did not improve from 0.41323\n",
      "250/250 [==============================] - 133s 533ms/step - loss: 0.5722 - accuracy: 0.8095 - val_loss: 0.4378 - val_accuracy: 0.8438\n",
      "Epoch 26/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4950 - accuracy: 0.8285\n",
      "Epoch 26: val_loss did not improve from 0.41323\n",
      "250/250 [==============================] - 161s 646ms/step - loss: 0.4950 - accuracy: 0.8285 - val_loss: 0.4746 - val_accuracy: 0.8411\n",
      "Epoch 27/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5040 - accuracy: 0.8205\n",
      "Epoch 27: val_loss did not improve from 0.41323\n",
      "250/250 [==============================] - 161s 643ms/step - loss: 0.5040 - accuracy: 0.8205 - val_loss: 0.4371 - val_accuracy: 0.8620\n",
      "Epoch 28/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4818 - accuracy: 0.8380\n",
      "Epoch 28: val_loss did not improve from 0.41323\n",
      "250/250 [==============================] - 149s 594ms/step - loss: 0.4818 - accuracy: 0.8380 - val_loss: 0.4392 - val_accuracy: 0.8464\n",
      "Epoch 29/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4523 - accuracy: 0.8475\n",
      "Epoch 29: val_loss did not improve from 0.41323\n",
      "250/250 [==============================] - 154s 616ms/step - loss: 0.4523 - accuracy: 0.8475 - val_loss: 0.4965 - val_accuracy: 0.8333\n",
      "Epoch 30/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4197 - accuracy: 0.8555\n",
      "Epoch 30: val_loss did not improve from 0.41323\n",
      "250/250 [==============================] - 161s 644ms/step - loss: 0.4197 - accuracy: 0.8555 - val_loss: 0.4776 - val_accuracy: 0.8385\n",
      "Epoch 31/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4499 - accuracy: 0.8460\n",
      "Epoch 31: val_loss did not improve from 0.41323\n",
      "250/250 [==============================] - 161s 646ms/step - loss: 0.4499 - accuracy: 0.8460 - val_loss: 0.4780 - val_accuracy: 0.8464\n",
      "Epoch 32/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4112 - accuracy: 0.8635\n",
      "Epoch 32: val_loss improved from 0.41323 to 0.36938, saving model to checkpoints\\\n",
      "250/250 [==============================] - 155s 621ms/step - loss: 0.4112 - accuracy: 0.8635 - val_loss: 0.3694 - val_accuracy: 0.8750\n",
      "Epoch 33/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4204 - accuracy: 0.8605\n",
      "Epoch 33: val_loss did not improve from 0.36938\n",
      "250/250 [==============================] - 161s 645ms/step - loss: 0.4204 - accuracy: 0.8605 - val_loss: 0.4234 - val_accuracy: 0.8568\n",
      "Epoch 34/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3796 - accuracy: 0.8700\n",
      "Epoch 34: val_loss improved from 0.36938 to 0.33707, saving model to checkpoints\\\n",
      "250/250 [==============================] - 161s 645ms/step - loss: 0.3796 - accuracy: 0.8700 - val_loss: 0.3371 - val_accuracy: 0.9010\n",
      "Epoch 35/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3724 - accuracy: 0.8750\n",
      "Epoch 35: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 644ms/step - loss: 0.3724 - accuracy: 0.8750 - val_loss: 0.3805 - val_accuracy: 0.8724\n",
      "Epoch 36/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3446 - accuracy: 0.8820\n",
      "Epoch 36: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 645ms/step - loss: 0.3446 - accuracy: 0.8820 - val_loss: 0.4879 - val_accuracy: 0.8438\n",
      "Epoch 37/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.8785\n",
      "Epoch 37: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 646ms/step - loss: 0.3602 - accuracy: 0.8785 - val_loss: 0.3859 - val_accuracy: 0.8672\n",
      "Epoch 38/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3329 - accuracy: 0.8840\n",
      "Epoch 38: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 149s 598ms/step - loss: 0.3329 - accuracy: 0.8840 - val_loss: 0.4022 - val_accuracy: 0.8776\n",
      "Epoch 39/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3130 - accuracy: 0.8975\n",
      "Epoch 39: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 162s 648ms/step - loss: 0.3130 - accuracy: 0.8975 - val_loss: 0.4274 - val_accuracy: 0.8568\n",
      "Epoch 40/100000\n",
      "209/250 [========================>.....] - ETA: 24s - loss: 0.3136 - accuracy: 0.8941"
     ]
    }
   ],
   "source": [
    "# the form_logits means the loss function has the 'softmax' buillt in.  This approach is numerically more stable\n",
    "# than including the softmax activation on the last layer of the classifier\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=baseline_config.learning_rate), \n",
    "              metrics=[\"accuracy\"],\n",
    "              )\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='checkpoints/',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "model.fit(train_dataset_b, \n",
    "          validation_data=validation_dataset_b,\n",
    "          steps_per_epoch=250,\n",
    "          callbacks=[model_checkpoint_callback],\n",
    "          epochs=baseline_config.max_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2848388c1d7df64c5912f8c74b12cb2f63a5fbb869f66edadd9f1eda580b6df3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
