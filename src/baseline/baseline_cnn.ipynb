{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# basic imports\n",
    "import tensorflow as tf \n",
    "import torch\n",
    "import io\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.utils import dataset_utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import baseline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12384 files belonging to 5 classes.\n",
      "Found 487 files belonging to 5 classes.\n",
      "Found 384 files belonging to 5 classes.\n",
      "class names:  ['brant', 'jabwar', 'sheowl', 'spodov', 'wiltur']\n"
     ]
    }
   ],
   "source": [
    "def paths_and_labels_to_dataset(image_paths,labels,num_classes):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    img_ds = path_ds.map(\n",
    "        lambda path: tf.io.read_file(path), \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    label_ds = dataset_utils.labels_to_dataset(\n",
    "        labels, \n",
    "        'categorical', \n",
    "        num_classes)\n",
    "    img_ds = tf.data.Dataset.zip((img_ds, label_ds))\n",
    "    return img_ds\n",
    "\n",
    "def create_dataset(subset):\n",
    "    image_paths, labels, class_names = dataset_utils.index_directory(\n",
    "            baseline_config.dataset_path + subset,\n",
    "            labels=\"inferred\",\n",
    "            formats=('.pt'),\n",
    "            class_names=None,\n",
    "            shuffle=False,\n",
    "            seed=42,\n",
    "            follow_links=False)\n",
    "\n",
    "    dataset = paths_and_labels_to_dataset(\n",
    "        image_paths=image_paths,\n",
    "        labels=labels,\n",
    "        num_classes=len(class_names))\n",
    "    \n",
    "    return dataset, class_names\n",
    "\n",
    "train_dataset, class_names = create_dataset('TRAIN/')\n",
    "test_dataset, _            = create_dataset('TEST/')\n",
    "validation_dataset, _      = create_dataset('VALIDATION/')\n",
    "print(\"class names: \", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_transforms(image,label):\n",
    "    image = tf.io.parse_tensor(image, tf.float32)\n",
    "    image = tf.expand_dims(image, -1)\n",
    "    image = tf.repeat(image, 3, 2)\n",
    "    return image,label\n",
    "\n",
    "train_dataset_b = ( \n",
    "                  train_dataset\n",
    "                  .shuffle(20000)\n",
    "                  .map(dataset_transforms)\n",
    "                  .batch(baseline_config.batch_size)\n",
    "                  .cache()\n",
    "                  .repeat()            \n",
    "                )\n",
    "\n",
    "validation_dataset_b = ( \n",
    "                  validation_dataset\n",
    "                  .map(dataset_transforms)\n",
    "                  .batch(baseline_config.batch_size)\n",
    "                  .cache()\n",
    "                )\n",
    "\n",
    "test_dataset_b = ( \n",
    "                  test_dataset\n",
    "                  .map(dataset_transforms)\n",
    "                  .batch(baseline_config.batch_size)\n",
    "                  .cache()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 313, 128, 3) (8, 5)\n"
     ]
    }
   ],
   "source": [
    "for item,lbl in train_dataset_b.take(1):\n",
    "    print(item.shape, lbl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resizing_1 (Resizing)       (None, 480, 480, 3)       0         \n",
      "                                                                 \n",
      " keras_layer_1 (KerasLayer)  (None, 1280)              53150388  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                40992     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,191,545\n",
      "Trainable params: 52,899,513\n",
      "Non-trainable params: 292,032\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build a really simple classification model using a pre-training Efficientnet V2\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        # use the model as a feature generator only\n",
    "        # need to resize here, as the efficientnet_v2_imagenet21k_s model expects it\n",
    "        tf.keras.layers.InputLayer(input_shape=(313,128,3)),\n",
    "        tf.keras.layers.Resizing(480, 480, interpolation=\"lanczos5\", crop_to_aspect_ratio=False),\n",
    "        \n",
    "        # downloaded from: https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\n",
    "        hub.KerasLayer(\"imagenet_efficientnet_v2_imagenet21k_m_feature_vector_2\", True),        \n",
    "\n",
    "        # add the classification layer here       \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dropout(0.60),\n",
    "        layers.Dense(len(class_names), activation=None),\n",
    "    ]\n",
    ")\n",
    "# need to tell the model what the input shape is\n",
    "model.build([None, 313, 128, 3])\n",
    "\n",
    "# show the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100000\n",
      "target shape (None, 5)\n",
      "output shape (None, 5)\n",
      "target shape (None, 5)\n",
      "output shape (None, 5)\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.9223 - accuracy: 0.2390target shape (None, 5)\n",
      "output shape (None, 5)\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.44631, saving model to checkpoints\\\n",
      "250/250 [==============================] - 139s 450ms/step - loss: 1.9223 - accuracy: 0.2390 - val_loss: 1.4463 - val_accuracy: 0.4010\n",
      "Epoch 2/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.5865 - accuracy: 0.3110\n",
      "Epoch 2: val_loss improved from 1.44631 to 1.33470, saving model to checkpoints\\\n",
      "250/250 [==============================] - 150s 599ms/step - loss: 1.5865 - accuracy: 0.3110 - val_loss: 1.3347 - val_accuracy: 0.5521\n",
      "Epoch 3/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.4894 - accuracy: 0.3640\n",
      "Epoch 3: val_loss improved from 1.33470 to 1.27570, saving model to checkpoints\\\n",
      "250/250 [==============================] - 172s 685ms/step - loss: 1.4894 - accuracy: 0.3640 - val_loss: 1.2757 - val_accuracy: 0.5547\n",
      "Epoch 4/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.4345 - accuracy: 0.3835\n",
      "Epoch 4: val_loss improved from 1.27570 to 1.11624, saving model to checkpoints\\\n",
      "250/250 [==============================] - 166s 663ms/step - loss: 1.4345 - accuracy: 0.3835 - val_loss: 1.1162 - val_accuracy: 0.6302\n",
      "Epoch 5/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.3493 - accuracy: 0.4250\n",
      "Epoch 5: val_loss improved from 1.11624 to 1.09242, saving model to checkpoints\\\n",
      "250/250 [==============================] - 167s 668ms/step - loss: 1.3493 - accuracy: 0.4250 - val_loss: 1.0924 - val_accuracy: 0.6198\n",
      "Epoch 6/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.3000 - accuracy: 0.4730\n",
      "Epoch 6: val_loss improved from 1.09242 to 0.99904, saving model to checkpoints\\\n",
      "250/250 [==============================] - 153s 613ms/step - loss: 1.3000 - accuracy: 0.4730 - val_loss: 0.9990 - val_accuracy: 0.6693\n",
      "Epoch 7/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.2121 - accuracy: 0.5100\n",
      "Epoch 7: val_loss improved from 0.99904 to 0.91800, saving model to checkpoints\\\n",
      "250/250 [==============================] - 164s 656ms/step - loss: 1.2121 - accuracy: 0.5100 - val_loss: 0.9180 - val_accuracy: 0.6667\n",
      "Epoch 8/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.1556 - accuracy: 0.5470\n",
      "Epoch 8: val_loss improved from 0.91800 to 0.87128, saving model to checkpoints\\\n",
      "250/250 [==============================] - 156s 621ms/step - loss: 1.1556 - accuracy: 0.5470 - val_loss: 0.8713 - val_accuracy: 0.6849\n",
      "Epoch 9/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.0719 - accuracy: 0.5790\n",
      "Epoch 9: val_loss improved from 0.87128 to 0.76360, saving model to checkpoints\\\n",
      "250/250 [==============================] - 159s 633ms/step - loss: 1.0719 - accuracy: 0.5790 - val_loss: 0.7636 - val_accuracy: 0.7500\n",
      "Epoch 10/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.0277 - accuracy: 0.6025\n",
      "Epoch 10: val_loss improved from 0.76360 to 0.65673, saving model to checkpoints\\\n",
      "250/250 [==============================] - 163s 652ms/step - loss: 1.0277 - accuracy: 0.6025 - val_loss: 0.6567 - val_accuracy: 0.8099\n",
      "Epoch 11/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.0355 - accuracy: 0.5970\n",
      "Epoch 11: val_loss improved from 0.65673 to 0.63839, saving model to checkpoints\\\n",
      "250/250 [==============================] - 160s 641ms/step - loss: 1.0355 - accuracy: 0.5970 - val_loss: 0.6384 - val_accuracy: 0.7969\n",
      "Epoch 12/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.9695 - accuracy: 0.6355\n",
      "Epoch 12: val_loss did not improve from 0.63839\n",
      "250/250 [==============================] - 161s 644ms/step - loss: 0.9695 - accuracy: 0.6355 - val_loss: 0.6681 - val_accuracy: 0.7734\n",
      "Epoch 13/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.9106 - accuracy: 0.6615\n",
      "Epoch 13: val_loss improved from 0.63839 to 0.60744, saving model to checkpoints\\\n",
      "250/250 [==============================] - 155s 620ms/step - loss: 0.9106 - accuracy: 0.6615 - val_loss: 0.6074 - val_accuracy: 0.7865\n",
      "Epoch 14/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.8653 - accuracy: 0.6790\n",
      "Epoch 14: val_loss improved from 0.60744 to 0.58464, saving model to checkpoints\\\n",
      "250/250 [==============================] - 161s 642ms/step - loss: 0.8653 - accuracy: 0.6790 - val_loss: 0.5846 - val_accuracy: 0.8047\n",
      "Epoch 15/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.8066 - accuracy: 0.6980\n",
      "Epoch 15: val_loss improved from 0.58464 to 0.56083, saving model to checkpoints\\\n",
      "250/250 [==============================] - 166s 664ms/step - loss: 0.8066 - accuracy: 0.6980 - val_loss: 0.5608 - val_accuracy: 0.8151\n",
      "Epoch 16/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.8223 - accuracy: 0.6890\n",
      "Epoch 16: val_loss did not improve from 0.56083\n",
      "250/250 [==============================] - 162s 646ms/step - loss: 0.8223 - accuracy: 0.6890 - val_loss: 0.6200 - val_accuracy: 0.7760\n",
      "Epoch 17/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.7507 - accuracy: 0.7155\n",
      "Epoch 17: val_loss improved from 0.56083 to 0.51453, saving model to checkpoints\\\n",
      "250/250 [==============================] - 166s 665ms/step - loss: 0.7507 - accuracy: 0.7155 - val_loss: 0.5145 - val_accuracy: 0.8203\n",
      "Epoch 18/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.7190 - accuracy: 0.7380\n",
      "Epoch 18: val_loss did not improve from 0.51453\n",
      "250/250 [==============================] - 162s 646ms/step - loss: 0.7190 - accuracy: 0.7380 - val_loss: 0.5891 - val_accuracy: 0.7839\n",
      "Epoch 19/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.7250 - accuracy: 0.7410\n",
      "Epoch 19: val_loss did not improve from 0.51453\n",
      "250/250 [==============================] - 158s 631ms/step - loss: 0.7250 - accuracy: 0.7410 - val_loss: 0.5771 - val_accuracy: 0.7995\n",
      "Epoch 20/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6696 - accuracy: 0.7640\n",
      "Epoch 20: val_loss improved from 0.51453 to 0.50409, saving model to checkpoints\\\n",
      "250/250 [==============================] - 166s 665ms/step - loss: 0.6696 - accuracy: 0.7640 - val_loss: 0.5041 - val_accuracy: 0.8203\n",
      "Epoch 21/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6512 - accuracy: 0.7600\n",
      "Epoch 21: val_loss improved from 0.50409 to 0.49782, saving model to checkpoints\\\n",
      "250/250 [==============================] - 167s 667ms/step - loss: 0.6512 - accuracy: 0.7600 - val_loss: 0.4978 - val_accuracy: 0.8359\n",
      "Epoch 22/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6135 - accuracy: 0.7825\n",
      "Epoch 22: val_loss improved from 0.49782 to 0.47786, saving model to checkpoints\\\n",
      "250/250 [==============================] - 166s 664ms/step - loss: 0.6135 - accuracy: 0.7825 - val_loss: 0.4779 - val_accuracy: 0.8385\n",
      "Epoch 23/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5503 - accuracy: 0.8115\n",
      "Epoch 23: val_loss did not improve from 0.47786\n",
      "250/250 [==============================] - 162s 646ms/step - loss: 0.5503 - accuracy: 0.8115 - val_loss: 0.4818 - val_accuracy: 0.8333\n",
      "Epoch 24/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5686 - accuracy: 0.7935\n",
      "Epoch 24: val_loss improved from 0.47786 to 0.41323, saving model to checkpoints\\\n",
      "250/250 [==============================] - 159s 638ms/step - loss: 0.5686 - accuracy: 0.7935 - val_loss: 0.4132 - val_accuracy: 0.8568\n",
      "Epoch 25/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5722 - accuracy: 0.8095\n",
      "Epoch 25: val_loss did not improve from 0.41323\n",
      "250/250 [==============================] - 133s 533ms/step - loss: 0.5722 - accuracy: 0.8095 - val_loss: 0.4378 - val_accuracy: 0.8438\n",
      "Epoch 26/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4950 - accuracy: 0.8285\n",
      "Epoch 26: val_loss did not improve from 0.41323\n",
      "250/250 [==============================] - 161s 646ms/step - loss: 0.4950 - accuracy: 0.8285 - val_loss: 0.4746 - val_accuracy: 0.8411\n",
      "Epoch 27/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5040 - accuracy: 0.8205\n",
      "Epoch 27: val_loss did not improve from 0.41323\n",
      "250/250 [==============================] - 161s 643ms/step - loss: 0.5040 - accuracy: 0.8205 - val_loss: 0.4371 - val_accuracy: 0.8620\n",
      "Epoch 28/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4818 - accuracy: 0.8380\n",
      "Epoch 28: val_loss did not improve from 0.41323\n",
      "250/250 [==============================] - 149s 594ms/step - loss: 0.4818 - accuracy: 0.8380 - val_loss: 0.4392 - val_accuracy: 0.8464\n",
      "Epoch 29/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4523 - accuracy: 0.8475\n",
      "Epoch 29: val_loss did not improve from 0.41323\n",
      "250/250 [==============================] - 154s 616ms/step - loss: 0.4523 - accuracy: 0.8475 - val_loss: 0.4965 - val_accuracy: 0.8333\n",
      "Epoch 30/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4197 - accuracy: 0.8555\n",
      "Epoch 30: val_loss did not improve from 0.41323\n",
      "250/250 [==============================] - 161s 644ms/step - loss: 0.4197 - accuracy: 0.8555 - val_loss: 0.4776 - val_accuracy: 0.8385\n",
      "Epoch 31/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4499 - accuracy: 0.8460\n",
      "Epoch 31: val_loss did not improve from 0.41323\n",
      "250/250 [==============================] - 161s 646ms/step - loss: 0.4499 - accuracy: 0.8460 - val_loss: 0.4780 - val_accuracy: 0.8464\n",
      "Epoch 32/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4112 - accuracy: 0.8635\n",
      "Epoch 32: val_loss improved from 0.41323 to 0.36938, saving model to checkpoints\\\n",
      "250/250 [==============================] - 155s 621ms/step - loss: 0.4112 - accuracy: 0.8635 - val_loss: 0.3694 - val_accuracy: 0.8750\n",
      "Epoch 33/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4204 - accuracy: 0.8605\n",
      "Epoch 33: val_loss did not improve from 0.36938\n",
      "250/250 [==============================] - 161s 645ms/step - loss: 0.4204 - accuracy: 0.8605 - val_loss: 0.4234 - val_accuracy: 0.8568\n",
      "Epoch 34/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3796 - accuracy: 0.8700\n",
      "Epoch 34: val_loss improved from 0.36938 to 0.33707, saving model to checkpoints\\\n",
      "250/250 [==============================] - 161s 645ms/step - loss: 0.3796 - accuracy: 0.8700 - val_loss: 0.3371 - val_accuracy: 0.9010\n",
      "Epoch 35/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3724 - accuracy: 0.8750\n",
      "Epoch 35: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 644ms/step - loss: 0.3724 - accuracy: 0.8750 - val_loss: 0.3805 - val_accuracy: 0.8724\n",
      "Epoch 36/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3446 - accuracy: 0.8820\n",
      "Epoch 36: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 645ms/step - loss: 0.3446 - accuracy: 0.8820 - val_loss: 0.4879 - val_accuracy: 0.8438\n",
      "Epoch 37/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.8785\n",
      "Epoch 37: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 646ms/step - loss: 0.3602 - accuracy: 0.8785 - val_loss: 0.3859 - val_accuracy: 0.8672\n",
      "Epoch 38/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3329 - accuracy: 0.8840\n",
      "Epoch 38: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 149s 598ms/step - loss: 0.3329 - accuracy: 0.8840 - val_loss: 0.4022 - val_accuracy: 0.8776\n",
      "Epoch 39/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3130 - accuracy: 0.8975\n",
      "Epoch 39: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 162s 648ms/step - loss: 0.3130 - accuracy: 0.8975 - val_loss: 0.4274 - val_accuracy: 0.8568\n",
      "Epoch 40/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3176 - accuracy: 0.8925\n",
      "Epoch 40: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 163s 652ms/step - loss: 0.3176 - accuracy: 0.8925 - val_loss: 0.4062 - val_accuracy: 0.8594\n",
      "Epoch 41/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3068 - accuracy: 0.8895\n",
      "Epoch 41: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 643ms/step - loss: 0.3068 - accuracy: 0.8895 - val_loss: 0.4884 - val_accuracy: 0.8385\n",
      "Epoch 42/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2958 - accuracy: 0.8935\n",
      "Epoch 42: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 160s 639ms/step - loss: 0.2958 - accuracy: 0.8935 - val_loss: 0.4505 - val_accuracy: 0.8776\n",
      "Epoch 43/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.9075\n",
      "Epoch 43: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 158s 633ms/step - loss: 0.2864 - accuracy: 0.9075 - val_loss: 0.3751 - val_accuracy: 0.8880\n",
      "Epoch 44/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3099 - accuracy: 0.8945\n",
      "Epoch 44: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 158s 634ms/step - loss: 0.3099 - accuracy: 0.8945 - val_loss: 0.3875 - val_accuracy: 0.8828\n",
      "Epoch 45/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2888 - accuracy: 0.9035\n",
      "Epoch 45: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 159s 635ms/step - loss: 0.2888 - accuracy: 0.9035 - val_loss: 0.4572 - val_accuracy: 0.8724\n",
      "Epoch 46/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2602 - accuracy: 0.9125\n",
      "Epoch 46: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 159s 635ms/step - loss: 0.2602 - accuracy: 0.9125 - val_loss: 0.3949 - val_accuracy: 0.8672\n",
      "Epoch 47/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9130\n",
      "Epoch 47: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 159s 635ms/step - loss: 0.2543 - accuracy: 0.9130 - val_loss: 0.5151 - val_accuracy: 0.8411\n",
      "Epoch 48/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2294 - accuracy: 0.9255\n",
      "Epoch 48: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 159s 636ms/step - loss: 0.2294 - accuracy: 0.9255 - val_loss: 0.5310 - val_accuracy: 0.8411\n",
      "Epoch 49/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2140 - accuracy: 0.9275\n",
      "Epoch 49: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 159s 637ms/step - loss: 0.2140 - accuracy: 0.9275 - val_loss: 0.5661 - val_accuracy: 0.8359\n",
      "Epoch 50/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.9235\n",
      "Epoch 50: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 159s 635ms/step - loss: 0.2491 - accuracy: 0.9235 - val_loss: 0.6331 - val_accuracy: 0.8542\n",
      "Epoch 51/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2233 - accuracy: 0.9220\n",
      "Epoch 51: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 159s 637ms/step - loss: 0.2233 - accuracy: 0.9220 - val_loss: 0.5103 - val_accuracy: 0.8307\n",
      "Epoch 52/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2236 - accuracy: 0.9230\n",
      "Epoch 52: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 644ms/step - loss: 0.2236 - accuracy: 0.9230 - val_loss: 0.4202 - val_accuracy: 0.8646\n",
      "Epoch 53/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.9205\n",
      "Epoch 53: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 645ms/step - loss: 0.2426 - accuracy: 0.9205 - val_loss: 0.4577 - val_accuracy: 0.8620\n",
      "Epoch 54/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1961 - accuracy: 0.9410\n",
      "Epoch 54: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 162s 647ms/step - loss: 0.1961 - accuracy: 0.9410 - val_loss: 0.4792 - val_accuracy: 0.8542\n",
      "Epoch 55/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2252 - accuracy: 0.9285\n",
      "Epoch 55: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 162s 648ms/step - loss: 0.2252 - accuracy: 0.9285 - val_loss: 0.3544 - val_accuracy: 0.8958\n",
      "Epoch 56/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1972 - accuracy: 0.9310\n",
      "Epoch 56: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 645ms/step - loss: 0.1972 - accuracy: 0.9310 - val_loss: 0.5293 - val_accuracy: 0.8307\n",
      "Epoch 57/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1891 - accuracy: 0.9425\n",
      "Epoch 57: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 162s 647ms/step - loss: 0.1891 - accuracy: 0.9425 - val_loss: 0.4848 - val_accuracy: 0.8646\n",
      "Epoch 58/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 0.9230\n",
      "Epoch 58: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 162s 647ms/step - loss: 0.2204 - accuracy: 0.9230 - val_loss: 0.5088 - val_accuracy: 0.8698\n",
      "Epoch 59/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1900 - accuracy: 0.9415\n",
      "Epoch 59: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 162s 646ms/step - loss: 0.1900 - accuracy: 0.9415 - val_loss: 0.5067 - val_accuracy: 0.8802\n",
      "Epoch 60/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1889 - accuracy: 0.9355\n",
      "Epoch 60: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 162s 646ms/step - loss: 0.1889 - accuracy: 0.9355 - val_loss: 0.5689 - val_accuracy: 0.8594\n",
      "Epoch 61/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1788 - accuracy: 0.9410\n",
      "Epoch 61: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 162s 647ms/step - loss: 0.1788 - accuracy: 0.9410 - val_loss: 0.6083 - val_accuracy: 0.8464\n",
      "Epoch 62/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1903 - accuracy: 0.9440\n",
      "Epoch 62: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 646ms/step - loss: 0.1903 - accuracy: 0.9440 - val_loss: 0.5968 - val_accuracy: 0.8307\n",
      "Epoch 63/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1630 - accuracy: 0.9445\n",
      "Epoch 63: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 645ms/step - loss: 0.1630 - accuracy: 0.9445 - val_loss: 0.5170 - val_accuracy: 0.8724\n",
      "Epoch 64/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1795 - accuracy: 0.9410\n",
      "Epoch 64: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 162s 646ms/step - loss: 0.1795 - accuracy: 0.9410 - val_loss: 0.5243 - val_accuracy: 0.8672\n",
      "Epoch 65/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1526 - accuracy: 0.9580\n",
      "Epoch 65: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 646ms/step - loss: 0.1526 - accuracy: 0.9580 - val_loss: 0.4869 - val_accuracy: 0.8672\n",
      "Epoch 66/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.9460\n",
      "Epoch 66: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 645ms/step - loss: 0.1644 - accuracy: 0.9460 - val_loss: 0.5851 - val_accuracy: 0.8438\n",
      "Epoch 67/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1352 - accuracy: 0.9550\n",
      "Epoch 67: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 162s 646ms/step - loss: 0.1352 - accuracy: 0.9550 - val_loss: 0.8767 - val_accuracy: 0.8021\n",
      "Epoch 68/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1685 - accuracy: 0.9440\n",
      "Epoch 68: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 162s 647ms/step - loss: 0.1685 - accuracy: 0.9440 - val_loss: 0.5102 - val_accuracy: 0.8672\n",
      "Epoch 69/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1450 - accuracy: 0.9510\n",
      "Epoch 69: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 644ms/step - loss: 0.1450 - accuracy: 0.9510 - val_loss: 0.5455 - val_accuracy: 0.8646\n",
      "Epoch 70/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1450 - accuracy: 0.9460\n",
      "Epoch 70: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 646ms/step - loss: 0.1450 - accuracy: 0.9460 - val_loss: 0.7286 - val_accuracy: 0.8411\n",
      "Epoch 71/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1480 - accuracy: 0.9490\n",
      "Epoch 71: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 162s 646ms/step - loss: 0.1480 - accuracy: 0.9490 - val_loss: 0.5922 - val_accuracy: 0.8620\n",
      "Epoch 72/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1472 - accuracy: 0.9445\n",
      "Epoch 72: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 643ms/step - loss: 0.1472 - accuracy: 0.9445 - val_loss: 0.7496 - val_accuracy: 0.8099\n",
      "Epoch 73/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9555\n",
      "Epoch 73: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 644ms/step - loss: 0.1249 - accuracy: 0.9555 - val_loss: 0.8229 - val_accuracy: 0.8177\n",
      "Epoch 74/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1486 - accuracy: 0.9455\n",
      "Epoch 74: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 645ms/step - loss: 0.1486 - accuracy: 0.9455 - val_loss: 0.7813 - val_accuracy: 0.8047\n",
      "Epoch 75/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1465 - accuracy: 0.9585\n",
      "Epoch 75: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 643ms/step - loss: 0.1465 - accuracy: 0.9585 - val_loss: 0.5601 - val_accuracy: 0.8724\n",
      "Epoch 76/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.9505\n",
      "Epoch 76: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 642ms/step - loss: 0.1399 - accuracy: 0.9505 - val_loss: 0.5829 - val_accuracy: 0.8620\n",
      "Epoch 77/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1447 - accuracy: 0.9475\n",
      "Epoch 77: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 645ms/step - loss: 0.1447 - accuracy: 0.9475 - val_loss: 0.5873 - val_accuracy: 0.8464\n",
      "Epoch 78/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9575\n",
      "Epoch 78: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 643ms/step - loss: 0.1292 - accuracy: 0.9575 - val_loss: 0.5670 - val_accuracy: 0.8620\n",
      "Epoch 79/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 0.9625\n",
      "Epoch 79: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 642ms/step - loss: 0.1184 - accuracy: 0.9625 - val_loss: 0.7505 - val_accuracy: 0.8385\n",
      "Epoch 80/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9525\n",
      "Epoch 80: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 643ms/step - loss: 0.1388 - accuracy: 0.9525 - val_loss: 0.7152 - val_accuracy: 0.8359\n",
      "Epoch 81/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1583 - accuracy: 0.9440\n",
      "Epoch 81: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 645ms/step - loss: 0.1583 - accuracy: 0.9440 - val_loss: 0.8019 - val_accuracy: 0.8151\n",
      "Epoch 82/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1280 - accuracy: 0.9550\n",
      "Epoch 82: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 644ms/step - loss: 0.1280 - accuracy: 0.9550 - val_loss: 0.6910 - val_accuracy: 0.8385\n",
      "Epoch 83/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9545\n",
      "Epoch 83: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 646ms/step - loss: 0.1225 - accuracy: 0.9545 - val_loss: 0.7063 - val_accuracy: 0.8333\n",
      "Epoch 84/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.9600\n",
      "Epoch 84: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 645ms/step - loss: 0.1148 - accuracy: 0.9600 - val_loss: 0.6776 - val_accuracy: 0.8490\n",
      "Epoch 85/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1210 - accuracy: 0.9590\n",
      "Epoch 85: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 643ms/step - loss: 0.1210 - accuracy: 0.9590 - val_loss: 0.8122 - val_accuracy: 0.8203\n",
      "Epoch 86/100000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9560\n",
      "Epoch 86: val_loss did not improve from 0.33707\n",
      "250/250 [==============================] - 161s 643ms/step - loss: 0.1216 - accuracy: 0.9560 - val_loss: 0.7951 - val_accuracy: 0.8333\n",
      "Epoch 87/100000\n",
      "235/250 [===========================>..] - ETA: 9s - loss: 0.1329 - accuracy: 0.9511"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 16\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), \n\u001b[0;32m      4\u001b[0m               optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mbaseline_config\u001b[39m.\u001b[39mlearning_rate), \n\u001b[0;32m      5\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m      6\u001b[0m               )\n\u001b[0;32m      8\u001b[0m model_checkpoint_callback \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(\n\u001b[0;32m      9\u001b[0m     filepath\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcheckpoints/\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m     save_weights_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m     14\u001b[0m     save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 16\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_dataset_b, \n\u001b[0;32m     17\u001b[0m           validation_data\u001b[39m=\u001b[39;49mvalidation_dataset_b,\n\u001b[0;32m     18\u001b[0m           steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39m250\u001b[39;49m,\n\u001b[0;32m     19\u001b[0m           callbacks\u001b[39m=\u001b[39;49m[model_checkpoint_callback],\n\u001b[0;32m     20\u001b[0m           epochs\u001b[39m=\u001b[39;49mbaseline_config\u001b[39m.\u001b[39;49mmax_epoch)\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# the form_logits means the loss function has the 'softmax' buillt in.  This approach is numerically more stable\n",
    "# than including the softmax activation on the last layer of the classifier\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=baseline_config.learning_rate), \n",
    "              metrics=[\"accuracy\"],\n",
    "              )\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='checkpoints/',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "model.fit(train_dataset_b, \n",
    "          validation_data=validation_dataset_b,\n",
    "          steps_per_epoch=250,\n",
    "          callbacks=[model_checkpoint_callback],\n",
    "          epochs=baseline_config.max_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2848388c1d7df64c5912f8c74b12cb2f63a5fbb869f66edadd9f1eda580b6df3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
