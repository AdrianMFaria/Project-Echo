{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version     :  3.9.15\n",
      "TensorFlow Version :  2.10.1\n",
      "Pytorch Version    :  1.13.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# some basic libraries\n",
    "import numpy as np\n",
    "from numba import cuda \n",
    "import tensorflow as tf\n",
    "from numpy.lib.function_base import average\n",
    "from platform import python_version\n",
    "\n",
    "# ipython display\n",
    "from IPython.core.display import display\n",
    "\n",
    "# pytorch lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, sampler\n",
    "from torch.utils.data import Dataset\n",
    "from torchlibrosa.stft import STFT, ISTFT, magphase\n",
    "\n",
    "from keras.utils import dataset_utils\n",
    "\n",
    "# sklearn machine learning library\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score\n",
    "\n",
    "from htsat_utils import do_mixup, get_mix_lambda, do_mixup_label\n",
    "from htsat_utils import get_loss_func, d_prime, float32_to_int16\n",
    "\n",
    "# import the HTSAT model\n",
    "from htsat_model import HTSAT_Swin_Transformer \n",
    "\n",
    "# import project echo modules\n",
    "import baseline_config\n",
    "import echo_data_module\n",
    "import echo_module\n",
    "\n",
    "# print system information\n",
    "print('Python Version     : ', python_version())\n",
    "print('TensorFlow Version : ', tf.__version__)\n",
    "print('Pytorch Version    : ', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12384 files belonging to 5 classes.\n",
      "Found 487 files belonging to 5 classes.\n",
      "Found 384 files belonging to 5 classes.\n",
      "class names:  ['brant', 'jabwar', 'sheowl', 'spodov', 'wiltur']\n"
     ]
    }
   ],
   "source": [
    "def paths_and_labels_to_dataset(image_paths,labels,num_classes):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    img_ds = path_ds.map(\n",
    "        lambda path: tf.io.read_file(path), \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    label_ds = dataset_utils.labels_to_dataset(\n",
    "        labels, \n",
    "        'categorical', \n",
    "        num_classes)\n",
    "    img_ds = tf.data.Dataset.zip((img_ds, label_ds))\n",
    "    return img_ds\n",
    "\n",
    "def create_dataset(subset):\n",
    "    image_paths, labels, class_names = dataset_utils.index_directory(\n",
    "            baseline_config.dataset_path + subset,\n",
    "            labels=\"inferred\",\n",
    "            formats=('.pt'),\n",
    "            class_names=None,\n",
    "            shuffle=False,\n",
    "            seed=42,\n",
    "            follow_links=False)\n",
    "\n",
    "    dataset = paths_and_labels_to_dataset(\n",
    "        image_paths=image_paths,\n",
    "        labels=labels,\n",
    "        num_classes=len(class_names))\n",
    "    \n",
    "    return dataset, class_names\n",
    "\n",
    "train_dataset, class_names = create_dataset('TRAIN/')\n",
    "test_dataset, _            = create_dataset('TEST/')\n",
    "validation_dataset, _      = create_dataset('VALIDATION/')\n",
    "print(\"class names: \", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313, 128, 1) (5,)\n"
     ]
    }
   ],
   "source": [
    "def dataset_transforms(image,label):\n",
    "    image = tf.io.parse_tensor(image, tf.float32)\n",
    "    image = tf.expand_dims(image, -1)\n",
    "    return image,label\n",
    "\n",
    "train_dataset_b = ( \n",
    "                  train_dataset\n",
    "                  .shuffle(20000)\n",
    "                  .map(dataset_transforms)\n",
    "                  .cache()           \n",
    "                )\n",
    "\n",
    "validation_dataset_b = ( \n",
    "                  validation_dataset\n",
    "                  .map(dataset_transforms)\n",
    "                  .cache()\n",
    "                )\n",
    "\n",
    "test_dataset_b = ( \n",
    "                  test_dataset\n",
    "                  .map(dataset_transforms)\n",
    "                  .cache()\n",
    "                )\n",
    "\n",
    "for item,lbl in train_dataset_b.take(1):\n",
    "    print(item.shape, lbl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EchoDatasetMelspec(Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        \n",
    "        self.audio_dataset = []\n",
    "        \n",
    "        # just load it all into RAM for now\n",
    "        for item,lbl in dataset:\n",
    "            self.audio_dataset.append((item.numpy(), lbl.numpy()))\n",
    "        \n",
    "    # this shuffles the whole list of training samples\n",
    "    def shuffle_dataset(self):\n",
    "        print(\"\")\n",
    "\n",
    "    # get sample at location 'index'\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Load waveform and target of an audio clip.\n",
    "        Args:\n",
    "            index: the index number\n",
    "        Return: {\n",
    "            \"filename\": str,\n",
    "            \"waveform\": (clip_samples,),\n",
    "            \"target\": (classes_num,)\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        # retrieve the sample from the dataset\n",
    "        sample = self.audio_dataset[index]\n",
    "\n",
    "        melspec = sample[0]                      # T F C\n",
    "        # print(\"melspec.shape\",melspec.shape) \n",
    "        melspec = np.transpose(melspec, (2,0,1)) # C T F        \n",
    "        \n",
    "        target = sample[1]\n",
    "             \n",
    "        # return a dictionary with the sample data\n",
    "        return {\n",
    "            \"filename\": \"n/a\",\n",
    "            \"melspec\": melspec,\n",
    "            \"target\": np.argmax(target),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EchoDatasetMelspec(train_dataset_b)\n",
    "validation_dataset = EchoDatasetMelspec(validation_dataset_b)\n",
    "\n",
    "# don't need tensorflow anymore, need to free memory so pytorch can use it\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# get the number of available GPUs\n",
    "device_num = torch.cuda.device_count()\n",
    "\n",
    "# create the audio data set pipeline\n",
    "audio_pipeline = echo_data_module.EchoDataModule(train_dataset, validation_dataset, device_num)\n",
    "\n",
    "# checkpoint to record snapshots during training\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath = 'checkpoints/',\n",
    "    monitor = \"acc\",\n",
    "    filename='l-{epoch:d}-{acc:.3f}',\n",
    "    save_top_k = 5,\n",
    "    mode = \"max\",\n",
    ")\n",
    "\n",
    "# resume from here in the training\n",
    "checkpoint_resume = 'checkpoints/l-epoch=1-acc=0.607.ckpt'\n",
    "\n",
    "# construct the model trainer\n",
    "trainer = pl.Trainer(\n",
    "        deterministic=False,\n",
    "        default_root_dir = baseline_config.workspace,\n",
    "        gpus = device_num, \n",
    "        val_check_interval = 0.1,\n",
    "        max_epochs = baseline_config.max_epoch,\n",
    "        auto_lr_find = True,    \n",
    "        sync_batchnorm = True,\n",
    "        callbacks = [checkpoint_callback],\n",
    "        accelerator = \"ddp\" if device_num > 1 else None,\n",
    "        num_sanity_val_steps = 0,\n",
    "        resume_from_checkpoint = checkpoint_resume, \n",
    "        replace_sampler_ddp = False,\n",
    "        gradient_clip_val=1.0\n",
    "    )\n",
    "\n",
    "# construct the model\n",
    "sed_model = HTSAT_Swin_Transformer(\n",
    "        spec_size=baseline_config.htsat_spec_size,\n",
    "        patch_size=baseline_config.htsat_patch_size,\n",
    "        in_chans=1,\n",
    "        num_classes=baseline_config.classes_num,\n",
    "        window_size=baseline_config.htsat_window_size,\n",
    "        config = baseline_config,\n",
    "        depths = baseline_config.htsat_depth,\n",
    "        embed_dim = baseline_config.htsat_dim,\n",
    "        patch_stride=baseline_config.htsat_stride,\n",
    "        num_heads=baseline_config.htsat_num_head\n",
    "    )\n",
    "\n",
    "# wrapper to track metrics during training \n",
    "model = echo_module.EchoModule(\n",
    "        sed_model = sed_model, \n",
    "        config = baseline_config,\n",
    "        dataset = validation_dataset\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at checkpoints/l-epoch=1-acc=0.607.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                   | Params\n",
      "-----------------------------------------------------\n",
      "0 | sed_model | HTSAT_Swin_Transformer | 27.8 M\n",
      "-----------------------------------------------------\n",
      "27.5 M    Trainable params\n",
      "296 K     Non-trainable params\n",
      "27.8 M    Total params\n",
      "111.338   Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint file at checkpoints/l-epoch=1-acc=0.607.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: : 891it [00:02, 323.94it/s, loss=1.19, v_num=3, loss_step=1.190]\n",
      "Epoch 2: 100%|██████████| 507/507 [06:00<00:00,  1.41it/s, loss=1.26, v_num=3, loss_step=1.240, loss_epoch=1.190, acc=0.568]\n",
      "Epoch 3: 100%|██████████| 507/507 [06:00<00:00,  1.41it/s, loss=1.21, v_num=3, loss_step=1.160, loss_epoch=1.270, acc=0.622]\n",
      "Epoch 4: 100%|██████████| 507/507 [05:58<00:00,  1.41it/s, loss=1.18, v_num=3, loss_step=1.190, loss_epoch=1.220, acc=0.669]\n",
      "Epoch 5: 100%|██████████| 507/507 [06:13<00:00,  1.36it/s, loss=1.21, v_num=3, loss_step=1.270, loss_epoch=1.200, acc=0.701]\n",
      "Epoch 6: 100%|██████████| 507/507 [06:15<00:00,  1.35it/s, loss=1.13, v_num=3, loss_step=1.170, loss_epoch=1.180, acc=0.740]\n",
      "Epoch 7: 100%|██████████| 507/507 [05:51<00:00,  1.44it/s, loss=1.15, v_num=3, loss_step=1.110, loss_epoch=1.160, acc=0.609]\n",
      "Epoch 8: 100%|██████████| 507/507 [05:44<00:00,  1.47it/s, loss=1.14, v_num=3, loss_step=1.070, loss_epoch=1.140, acc=0.688]\n",
      "Epoch 9: 100%|██████████| 507/507 [05:50<00:00,  1.45it/s, loss=1.12, v_num=3, loss_step=1.110, loss_epoch=1.130, acc=0.724]\n",
      "Epoch 10: 100%|██████████| 507/507 [05:43<00:00,  1.47it/s, loss=1.08, v_num=3, loss_step=1.000, loss_epoch=1.110, acc=0.690]\n",
      "Epoch 11: 100%|██████████| 507/507 [05:52<00:00,  1.44it/s, loss=1.08, v_num=3, loss_step=1.000, loss_epoch=1.090, acc=0.742]\n",
      "Epoch 12: 100%|██████████| 507/507 [05:51<00:00,  1.44it/s, loss=1.06, v_num=3, loss_step=1.000, loss_epoch=1.070, acc=0.674]\n",
      "Epoch 13: 100%|██████████| 507/507 [05:49<00:00,  1.45it/s, loss=1.06, v_num=3, loss_step=1.030, loss_epoch=1.060, acc=0.737]\n",
      "Epoch 14: 100%|██████████| 507/507 [06:00<00:00,  1.41it/s, loss=1.06, v_num=3, loss_step=1.080, loss_epoch=1.060, acc=0.758]\n",
      "Epoch 15: 100%|██████████| 507/507 [05:45<00:00,  1.47it/s, loss=1.05, v_num=3, loss_step=1.000, loss_epoch=1.050, acc=0.674]\n",
      "Epoch 16: 100%|██████████| 507/507 [05:46<00:00,  1.46it/s, loss=1.04, v_num=3, loss_step=1.040, loss_epoch=1.050, acc=0.771]\n",
      "Epoch 17: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=1.04, v_num=3, loss_step=1.000, loss_epoch=1.040, acc=0.682]\n",
      "Epoch 18: 100%|██████████| 507/507 [05:44<00:00,  1.47it/s, loss=1.04, v_num=3, loss_step=1.030, loss_epoch=1.040, acc=0.721]\n",
      "Epoch 19: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=1.04, v_num=3, loss_step=1.030, loss_epoch=1.040, acc=0.750]\n",
      "Epoch 20: 100%|██████████| 507/507 [05:41<00:00,  1.48it/s, loss=1.02, v_num=3, loss_step=0.959, loss_epoch=1.030, acc=0.747] \n",
      "Epoch 21: 100%|██████████| 507/507 [05:53<00:00,  1.44it/s, loss=1, v_num=3, loss_step=1.020, loss_epoch=1.010, acc=0.745]    \n",
      "Epoch 22: 100%|██████████| 507/507 [05:52<00:00,  1.44it/s, loss=1.01, v_num=3, loss_step=0.943, loss_epoch=1.000, acc=0.703] \n",
      "Epoch 23: 100%|██████████| 507/507 [05:41<00:00,  1.49it/s, loss=1.01, v_num=3, loss_step=0.961, loss_epoch=1.000, acc=0.763] \n",
      "Epoch 24: 100%|██████████| 507/507 [05:37<00:00,  1.50it/s, loss=0.999, v_num=3, loss_step=0.993, loss_epoch=1.000, acc=0.701]\n",
      "Epoch 25: 100%|██████████| 507/507 [05:42<00:00,  1.48it/s, loss=1.01, v_num=3, loss_step=0.938, loss_epoch=1.000, acc=0.732] \n",
      "Epoch 26: 100%|██████████| 507/507 [05:42<00:00,  1.48it/s, loss=0.994, v_num=3, loss_step=0.920, loss_epoch=0.998, acc=0.760]\n",
      "Epoch 27: 100%|██████████| 507/507 [05:41<00:00,  1.49it/s, loss=0.999, v_num=3, loss_step=0.961, loss_epoch=0.996, acc=0.737]\n",
      "Epoch 28: 100%|██████████| 507/507 [05:39<00:00,  1.49it/s, loss=0.994, v_num=3, loss_step=0.940, loss_epoch=0.995, acc=0.740]\n",
      "Epoch 29: 100%|██████████| 507/507 [05:43<00:00,  1.47it/s, loss=0.994, v_num=3, loss_step=0.965, loss_epoch=0.992, acc=0.737]\n",
      "Epoch 30: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=1, v_num=3, loss_step=0.954, loss_epoch=0.987, acc=0.760]    \n",
      "Epoch 31: 100%|██████████| 507/507 [05:45<00:00,  1.47it/s, loss=0.984, v_num=3, loss_step=0.915, loss_epoch=0.993, acc=0.753]\n",
      "Epoch 32: 100%|██████████| 507/507 [05:45<00:00,  1.47it/s, loss=0.993, v_num=3, loss_step=0.925, loss_epoch=0.991, acc=0.734]\n",
      "Epoch 33: 100%|██████████| 507/507 [05:46<00:00,  1.47it/s, loss=0.996, v_num=3, loss_step=0.938, loss_epoch=0.987, acc=0.721]\n",
      "Epoch 34: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.988, v_num=3, loss_step=0.948, loss_epoch=0.990, acc=0.737]\n",
      "Epoch 35: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.979, v_num=3, loss_step=0.934, loss_epoch=0.989, acc=0.747]\n",
      "Epoch 36: 100%|██████████| 507/507 [05:41<00:00,  1.49it/s, loss=0.989, v_num=3, loss_step=0.947, loss_epoch=0.988, acc=0.737]\n",
      "Epoch 37: 100%|██████████| 507/507 [05:44<00:00,  1.47it/s, loss=0.994, v_num=3, loss_step=0.958, loss_epoch=0.980, acc=0.701]\n",
      "Epoch 38: 100%|██████████| 507/507 [05:44<00:00,  1.47it/s, loss=0.98, v_num=3, loss_step=0.956, loss_epoch=0.982, acc=0.688] \n",
      "Epoch 39: 100%|██████████| 507/507 [05:45<00:00,  1.47it/s, loss=0.971, v_num=3, loss_step=0.953, loss_epoch=0.979, acc=0.711]\n",
      "Epoch 40: 100%|██████████| 507/507 [05:44<00:00,  1.47it/s, loss=0.984, v_num=3, loss_step=0.922, loss_epoch=0.980, acc=0.763]\n",
      "Epoch 41: 100%|██████████| 507/507 [05:47<00:00,  1.46it/s, loss=0.969, v_num=3, loss_step=0.915, loss_epoch=0.979, acc=0.742]\n",
      "Epoch 42: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.972, v_num=3, loss_step=0.955, loss_epoch=0.975, acc=0.711]\n",
      "Epoch 43: 100%|██████████| 507/507 [05:36<00:00,  1.51it/s, loss=0.974, v_num=3, loss_step=0.959, loss_epoch=0.975, acc=0.737]\n",
      "Epoch 44: 100%|██████████| 507/507 [05:39<00:00,  1.49it/s, loss=0.98, v_num=3, loss_step=0.933, loss_epoch=0.974, acc=0.734] \n",
      "Epoch 45: 100%|██████████| 507/507 [05:43<00:00,  1.48it/s, loss=0.984, v_num=3, loss_step=0.912, loss_epoch=0.974, acc=0.716]\n",
      "Epoch 46: 100%|██████████| 507/507 [05:43<00:00,  1.48it/s, loss=0.978, v_num=3, loss_step=0.939, loss_epoch=0.974, acc=0.747]\n",
      "Epoch 47: 100%|██████████| 507/507 [05:42<00:00,  1.48it/s, loss=0.973, v_num=3, loss_step=0.919, loss_epoch=0.974, acc=0.698]\n",
      "Epoch 48: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.975, v_num=3, loss_step=0.946, loss_epoch=0.970, acc=0.711]\n",
      "Epoch 49: 100%|██████████| 507/507 [05:44<00:00,  1.47it/s, loss=0.97, v_num=3, loss_step=0.942, loss_epoch=0.969, acc=0.714] \n",
      "Epoch 50: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.964, v_num=3, loss_step=0.908, loss_epoch=0.967, acc=0.708]\n",
      "Epoch 51: 100%|██████████| 507/507 [05:46<00:00,  1.46it/s, loss=0.956, v_num=3, loss_step=0.943, loss_epoch=0.967, acc=0.758]\n",
      "Epoch 52: 100%|██████████| 507/507 [05:41<00:00,  1.49it/s, loss=0.968, v_num=3, loss_step=0.956, loss_epoch=0.966, acc=0.763]\n",
      "Epoch 53: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.98, v_num=3, loss_step=0.950, loss_epoch=0.963, acc=0.708] \n",
      "Epoch 54: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.961, v_num=3, loss_step=0.941, loss_epoch=0.963, acc=0.745]\n",
      "Epoch 55: 100%|██████████| 507/507 [05:43<00:00,  1.48it/s, loss=0.966, v_num=3, loss_step=0.912, loss_epoch=0.965, acc=0.690]\n",
      "Epoch 56: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.959, v_num=3, loss_step=0.922, loss_epoch=0.962, acc=0.711]\n",
      "Epoch 57: 100%|██████████| 507/507 [05:38<00:00,  1.50it/s, loss=0.962, v_num=3, loss_step=0.939, loss_epoch=0.961, acc=0.724]\n",
      "Epoch 58: 100%|██████████| 507/507 [05:53<00:00,  1.43it/s, loss=0.959, v_num=3, loss_step=0.914, loss_epoch=0.962, acc=0.773]\n",
      "Epoch 59: 100%|██████████| 507/507 [05:45<00:00,  1.47it/s, loss=0.959, v_num=3, loss_step=0.910, loss_epoch=0.961, acc=0.760]\n",
      "Epoch 60: 100%|██████████| 507/507 [05:46<00:00,  1.46it/s, loss=0.967, v_num=3, loss_step=0.943, loss_epoch=0.961, acc=0.708]\n",
      "Epoch 61: 100%|██████████| 507/507 [05:41<00:00,  1.48it/s, loss=0.947, v_num=3, loss_step=0.961, loss_epoch=0.957, acc=0.680]\n",
      "Epoch 62: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.958, v_num=3, loss_step=0.906, loss_epoch=0.960, acc=0.771]\n",
      "Epoch 63: 100%|██████████| 507/507 [05:41<00:00,  1.49it/s, loss=0.953, v_num=3, loss_step=0.907, loss_epoch=0.958, acc=0.698]\n",
      "Epoch 64: 100%|██████████| 507/507 [05:39<00:00,  1.49it/s, loss=0.955, v_num=3, loss_step=0.910, loss_epoch=0.957, acc=0.734]\n",
      "Epoch 65: 100%|██████████| 507/507 [05:42<00:00,  1.48it/s, loss=0.957, v_num=3, loss_step=0.926, loss_epoch=0.956, acc=0.760]\n",
      "Epoch 66: 100%|██████████| 507/507 [05:43<00:00,  1.48it/s, loss=0.957, v_num=3, loss_step=0.944, loss_epoch=0.956, acc=0.768]\n",
      "Epoch 67: 100%|██████████| 507/507 [05:43<00:00,  1.48it/s, loss=0.959, v_num=3, loss_step=0.934, loss_epoch=0.957, acc=0.734]\n",
      "Epoch 68: 100%|██████████| 507/507 [05:46<00:00,  1.47it/s, loss=0.948, v_num=3, loss_step=0.906, loss_epoch=0.955, acc=0.766]\n",
      "Epoch 69: 100%|██████████| 507/507 [05:48<00:00,  1.45it/s, loss=0.95, v_num=3, loss_step=0.957, loss_epoch=0.955, acc=0.721] \n",
      "Epoch 70: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.949, v_num=3, loss_step=0.928, loss_epoch=0.953, acc=0.755]\n",
      "Epoch 71: 100%|██████████| 507/507 [05:48<00:00,  1.46it/s, loss=0.949, v_num=3, loss_step=0.911, loss_epoch=0.951, acc=0.742]\n",
      "Epoch 72: 100%|██████████| 507/507 [05:41<00:00,  1.48it/s, loss=0.964, v_num=3, loss_step=0.936, loss_epoch=0.953, acc=0.688]\n",
      "Epoch 73: 100%|██████████| 507/507 [05:41<00:00,  1.49it/s, loss=0.946, v_num=3, loss_step=0.940, loss_epoch=0.952, acc=0.763]\n",
      "Epoch 74: 100%|██████████| 507/507 [05:39<00:00,  1.50it/s, loss=0.958, v_num=3, loss_step=0.936, loss_epoch=0.950, acc=0.737]\n",
      "Epoch 75: 100%|██████████| 507/507 [05:39<00:00,  1.49it/s, loss=0.949, v_num=3, loss_step=0.940, loss_epoch=0.955, acc=0.732]\n",
      "Epoch 76: 100%|██████████| 507/507 [05:35<00:00,  1.51it/s, loss=0.948, v_num=3, loss_step=0.906, loss_epoch=0.951, acc=0.760]\n",
      "Epoch 77: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.947, v_num=3, loss_step=0.933, loss_epoch=0.952, acc=0.760]\n",
      "Epoch 78: 100%|██████████| 507/507 [05:44<00:00,  1.47it/s, loss=0.957, v_num=3, loss_step=0.967, loss_epoch=0.948, acc=0.706]\n",
      "Epoch 79: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.952, v_num=3, loss_step=0.919, loss_epoch=0.951, acc=0.760]\n",
      "Epoch 80: 100%|██████████| 507/507 [05:43<00:00,  1.47it/s, loss=0.952, v_num=3, loss_step=0.978, loss_epoch=0.949, acc=0.763]\n",
      "Epoch 81: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.955, v_num=3, loss_step=0.911, loss_epoch=0.949, acc=0.758]\n",
      "Epoch 82: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.956, v_num=3, loss_step=0.974, loss_epoch=0.949, acc=0.750]\n",
      "Epoch 83: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.944, v_num=3, loss_step=0.962, loss_epoch=0.950, acc=0.786]\n",
      "Epoch 84: 100%|██████████| 507/507 [05:46<00:00,  1.46it/s, loss=0.946, v_num=3, loss_step=0.919, loss_epoch=0.947, acc=0.701]\n",
      "Epoch 85: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.946, v_num=3, loss_step=0.917, loss_epoch=0.945, acc=0.745]\n",
      "Epoch 86: 100%|██████████| 507/507 [05:38<00:00,  1.50it/s, loss=0.949, v_num=3, loss_step=0.928, loss_epoch=0.946, acc=0.773]\n",
      "Epoch 87: 100%|██████████| 507/507 [05:38<00:00,  1.50it/s, loss=0.953, v_num=3, loss_step=0.914, loss_epoch=0.945, acc=0.753]\n",
      "Epoch 88: 100%|██████████| 507/507 [05:39<00:00,  1.49it/s, loss=0.951, v_num=3, loss_step=0.947, loss_epoch=0.947, acc=0.753]\n",
      "Epoch 89: 100%|██████████| 507/507 [05:39<00:00,  1.49it/s, loss=0.953, v_num=3, loss_step=0.932, loss_epoch=0.948, acc=0.760]\n",
      "Epoch 90: 100%|██████████| 507/507 [05:42<00:00,  1.48it/s, loss=0.938, v_num=3, loss_step=0.916, loss_epoch=0.945, acc=0.729]\n",
      "Epoch 91: 100%|██████████| 507/507 [05:41<00:00,  1.49it/s, loss=0.944, v_num=3, loss_step=0.968, loss_epoch=0.947, acc=0.745]\n",
      "Epoch 92: 100%|██████████| 507/507 [05:39<00:00,  1.49it/s, loss=0.938, v_num=3, loss_step=0.941, loss_epoch=0.943, acc=0.745]\n",
      "Epoch 93: 100%|██████████| 507/507 [05:41<00:00,  1.49it/s, loss=0.945, v_num=3, loss_step=0.937, loss_epoch=0.945, acc=0.755]\n",
      "Epoch 94: 100%|██████████| 507/507 [05:42<00:00,  1.48it/s, loss=0.943, v_num=3, loss_step=0.919, loss_epoch=0.943, acc=0.742]\n",
      "Epoch 95: 100%|██████████| 507/507 [05:44<00:00,  1.47it/s, loss=0.944, v_num=3, loss_step=0.928, loss_epoch=0.945, acc=0.732]\n",
      "Epoch 96: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.942, v_num=3, loss_step=0.910, loss_epoch=0.946, acc=0.727]\n",
      "Epoch 97: 100%|██████████| 507/507 [05:07<00:00,  1.65it/s, loss=0.959, v_num=3, loss_step=0.937, loss_epoch=0.942, acc=0.753]\n",
      "Epoch 98: 100%|██████████| 507/507 [04:40<00:00,  1.81it/s, loss=0.946, v_num=3, loss_step=0.908, loss_epoch=0.944, acc=0.776]\n",
      "Epoch 99: 100%|██████████| 507/507 [05:19<00:00,  1.59it/s, loss=0.947, v_num=3, loss_step=0.935, loss_epoch=0.942, acc=0.729]\n",
      "Epoch 100: 100%|██████████| 507/507 [05:49<00:00,  1.45it/s, loss=0.941, v_num=3, loss_step=0.938, loss_epoch=0.944, acc=0.753]\n",
      "Epoch 101: 100%|██████████| 507/507 [05:44<00:00,  1.47it/s, loss=0.946, v_num=3, loss_step=0.908, loss_epoch=0.944, acc=0.763]\n",
      "Epoch 102: 100%|██████████| 507/507 [05:48<00:00,  1.46it/s, loss=0.94, v_num=3, loss_step=0.908, loss_epoch=0.944, acc=0.784] \n",
      "Epoch 103: 100%|██████████| 507/507 [05:42<00:00,  1.48it/s, loss=0.94, v_num=3, loss_step=0.936, loss_epoch=0.942, acc=0.750] \n",
      "Epoch 104: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.946, v_num=3, loss_step=0.942, loss_epoch=0.939, acc=0.703]\n",
      "Epoch 105: 100%|██████████| 507/507 [05:42<00:00,  1.48it/s, loss=0.947, v_num=3, loss_step=0.939, loss_epoch=0.941, acc=0.708]\n",
      "Epoch 106: 100%|██████████| 507/507 [05:39<00:00,  1.49it/s, loss=0.954, v_num=3, loss_step=0.979, loss_epoch=0.942, acc=0.693]\n",
      "Epoch 107: 100%|██████████| 507/507 [05:41<00:00,  1.49it/s, loss=0.946, v_num=3, loss_step=0.931, loss_epoch=0.940, acc=0.766]\n",
      "Epoch 108: 100%|██████████| 507/507 [05:44<00:00,  1.47it/s, loss=0.943, v_num=3, loss_step=0.934, loss_epoch=0.939, acc=0.727]\n",
      "Epoch 109: 100%|██████████| 507/507 [05:44<00:00,  1.47it/s, loss=0.947, v_num=3, loss_step=0.913, loss_epoch=0.941, acc=0.763]\n",
      "Epoch 110: 100%|██████████| 507/507 [05:27<00:00,  1.55it/s, loss=0.935, v_num=3, loss_step=0.940, loss_epoch=0.941, acc=0.768]\n",
      "Epoch 111: 100%|██████████| 507/507 [04:40<00:00,  1.81it/s, loss=0.946, v_num=3, loss_step=0.936, loss_epoch=0.939, acc=0.721]\n",
      "Epoch 112: 100%|██████████| 507/507 [04:39<00:00,  1.81it/s, loss=0.945, v_num=3, loss_step=0.946, loss_epoch=0.940, acc=0.789]\n",
      "Epoch 113: 100%|██████████| 507/507 [04:52<00:00,  1.74it/s, loss=0.939, v_num=3, loss_step=0.912, loss_epoch=0.938, acc=0.734]\n",
      "Epoch 114: 100%|██████████| 507/507 [04:42<00:00,  1.79it/s, loss=0.935, v_num=3, loss_step=0.927, loss_epoch=0.941, acc=0.740]\n",
      "Epoch 115: 100%|██████████| 507/507 [05:09<00:00,  1.64it/s, loss=0.945, v_num=3, loss_step=0.907, loss_epoch=0.938, acc=0.727]\n",
      "Epoch 116: 100%|██████████| 507/507 [05:24<00:00,  1.56it/s, loss=0.935, v_num=3, loss_step=0.907, loss_epoch=0.939, acc=0.758]\n",
      "Epoch 117: 100%|██████████| 507/507 [05:48<00:00,  1.45it/s, loss=0.947, v_num=3, loss_step=0.920, loss_epoch=0.938, acc=0.742]\n",
      "Epoch 118: 100%|██████████| 507/507 [05:51<00:00,  1.44it/s, loss=0.949, v_num=3, loss_step=0.944, loss_epoch=0.939, acc=0.807]\n",
      "Epoch 119: 100%|██████████| 507/507 [05:38<00:00,  1.50it/s, loss=0.935, v_num=3, loss_step=0.908, loss_epoch=0.938, acc=0.776]\n",
      "Epoch 120: 100%|██████████| 507/507 [05:39<00:00,  1.49it/s, loss=0.933, v_num=3, loss_step=0.905, loss_epoch=0.937, acc=0.740]\n",
      "Epoch 121: 100%|██████████| 507/507 [05:38<00:00,  1.50it/s, loss=0.938, v_num=3, loss_step=0.920, loss_epoch=0.936, acc=0.763]\n",
      "Epoch 122: 100%|██████████| 507/507 [05:37<00:00,  1.50it/s, loss=0.937, v_num=3, loss_step=0.925, loss_epoch=0.937, acc=0.729]\n",
      "Epoch 123: 100%|██████████| 507/507 [05:31<00:00,  1.53it/s, loss=0.933, v_num=3, loss_step=0.948, loss_epoch=0.938, acc=0.719]\n",
      "Epoch 124: 100%|██████████| 507/507 [05:09<00:00,  1.64it/s, loss=0.929, v_num=3, loss_step=0.911, loss_epoch=0.937, acc=0.734]\n",
      "Epoch 125: 100%|██████████| 507/507 [05:42<00:00,  1.48it/s, loss=0.933, v_num=3, loss_step=0.912, loss_epoch=0.937, acc=0.750]\n",
      "Epoch 126: 100%|██████████| 507/507 [05:38<00:00,  1.50it/s, loss=0.937, v_num=3, loss_step=0.908, loss_epoch=0.936, acc=0.740]\n",
      "Epoch 127: 100%|██████████| 507/507 [05:41<00:00,  1.48it/s, loss=0.935, v_num=3, loss_step=0.908, loss_epoch=0.937, acc=0.742]\n",
      "Epoch 128: 100%|██████████| 507/507 [05:44<00:00,  1.47it/s, loss=0.942, v_num=3, loss_step=0.913, loss_epoch=0.935, acc=0.729]\n",
      "Epoch 129: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.933, v_num=3, loss_step=0.921, loss_epoch=0.935, acc=0.758]\n",
      "Epoch 130: 100%|██████████| 507/507 [05:45<00:00,  1.47it/s, loss=0.936, v_num=3, loss_step=0.908, loss_epoch=0.934, acc=0.740]\n",
      "Epoch 131: 100%|██████████| 507/507 [05:35<00:00,  1.51it/s, loss=0.936, v_num=3, loss_step=0.936, loss_epoch=0.937, acc=0.732]\n",
      "Epoch 132: 100%|██████████| 507/507 [05:36<00:00,  1.51it/s, loss=0.931, v_num=3, loss_step=0.906, loss_epoch=0.934, acc=0.773]\n",
      "Epoch 133: 100%|██████████| 507/507 [05:48<00:00,  1.46it/s, loss=0.936, v_num=3, loss_step=0.908, loss_epoch=0.935, acc=0.737]\n",
      "Epoch 134: 100%|██████████| 507/507 [05:43<00:00,  1.48it/s, loss=0.933, v_num=3, loss_step=0.913, loss_epoch=0.933, acc=0.706]\n",
      "Epoch 135: 100%|██████████| 507/507 [05:04<00:00,  1.67it/s, loss=0.932, v_num=3, loss_step=0.940, loss_epoch=0.934, acc=0.786]\n",
      "Epoch 136: 100%|██████████| 507/507 [05:12<00:00,  1.62it/s, loss=0.939, v_num=3, loss_step=0.907, loss_epoch=0.934, acc=0.758]\n",
      "Epoch 137: 100%|██████████| 507/507 [05:25<00:00,  1.56it/s, loss=0.926, v_num=3, loss_step=0.908, loss_epoch=0.934, acc=0.779]\n",
      "Epoch 138: 100%|██████████| 507/507 [05:18<00:00,  1.59it/s, loss=0.942, v_num=3, loss_step=0.927, loss_epoch=0.932, acc=0.755]\n",
      "Epoch 139: 100%|██████████| 507/507 [05:05<00:00,  1.66it/s, loss=0.935, v_num=3, loss_step=0.910, loss_epoch=0.935, acc=0.776]\n",
      "Epoch 140: 100%|██████████| 507/507 [05:22<00:00,  1.57it/s, loss=0.93, v_num=3, loss_step=0.911, loss_epoch=0.932, acc=0.789] \n",
      "Epoch 141: 100%|██████████| 507/507 [05:39<00:00,  1.49it/s, loss=0.936, v_num=3, loss_step=0.911, loss_epoch=0.934, acc=0.732]\n",
      "Epoch 142: 100%|██████████| 507/507 [05:27<00:00,  1.55it/s, loss=0.934, v_num=3, loss_step=0.925, loss_epoch=0.932, acc=0.771]\n",
      "Epoch 143: 100%|██████████| 507/507 [05:15<00:00,  1.60it/s, loss=0.927, v_num=3, loss_step=0.937, loss_epoch=0.934, acc=0.737]\n",
      "Epoch 144: 100%|██████████| 507/507 [05:01<00:00,  1.68it/s, loss=0.935, v_num=3, loss_step=0.905, loss_epoch=0.932, acc=0.716]\n",
      "Epoch 145: 100%|██████████| 507/507 [05:24<00:00,  1.56it/s, loss=0.935, v_num=3, loss_step=0.909, loss_epoch=0.933, acc=0.763]\n",
      "Epoch 146: 100%|██████████| 507/507 [05:10<00:00,  1.63it/s, loss=0.929, v_num=3, loss_step=0.916, loss_epoch=0.932, acc=0.745]\n",
      "Epoch 147: 100%|██████████| 507/507 [05:32<00:00,  1.53it/s, loss=0.932, v_num=3, loss_step=0.905, loss_epoch=0.930, acc=0.745]\n",
      "Epoch 148: 100%|██████████| 507/507 [05:40<00:00,  1.49it/s, loss=0.94, v_num=3, loss_step=0.906, loss_epoch=0.932, acc=0.716] \n",
      "Epoch 149: 100%|██████████| 507/507 [05:52<00:00,  1.44it/s, loss=0.933, v_num=3, loss_step=0.941, loss_epoch=0.932, acc=0.779]\n",
      "Epoch 150:  62%|██████▏   | 316/507 [03:40<02:13,  1.43it/s, loss=0.922, v_num=3, loss_step=0.922, loss_epoch=0.931, acc=0.716]"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.fit(model, audio_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2848388c1d7df64c5912f8c74b12cb2f63a5fbb869f66edadd9f1eda580b6df3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
