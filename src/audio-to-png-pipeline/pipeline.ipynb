{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview\n",
    "\n",
    "Author: stephankokkas\n",
    "\n",
    "This notebook defines a pipeline that tasks an input directory of audio files and converts them to images using mel-spectrogram transofrmation and preprocessing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version     :  3.10.0\n",
      "TensorFlow Version :  2.11.0\n"
     ]
    }
   ],
   "source": [
    "# disable warnings to tidy up output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# some basic libraries \n",
    "from platform import python_version\n",
    "#import pandas as pd\n",
    "#import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# plot support\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tensorflow support\n",
    "import tensorflow as tf\n",
    "#import tensorflow_transform as tft\n",
    "import tensorflow_io as tfio\n",
    "#from tensorflow.contrib.framework.python.ops import audio_ops\n",
    "\n",
    "# scipy\n",
    "import scipy\n",
    "from pydub import AudioSegment, effects\n",
    "\n",
    "# turn off tensorflow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# turn off absl warnings\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "# print system information\n",
    "print('Python Version     : ', python_version())\n",
    "print('TensorFlow Version : ', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below code adapted from:\n",
    "# https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(123)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "random.seed(123)\n",
    "\n",
    "# The below set_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set system parameters\n",
    "DATASET_PATH  = '/Users/stephankokkas/Downloads/birdclef2022/'\n",
    "\n",
    "SAMPLE_RATE   = 32000   # all the samples are converted to bit rate of 32000 (Samples/Second)\n",
    "MIN_FREQUENCY = 16      # minimum frequency (Hz) for the Fast Fourier Transform related functions\n",
    "MAX_FREQUENCY = 4096*3  # minimum frequency (Hz) for the Fast Fourier Transform related functions\n",
    "HOP_LENGTH    = 128     # the number of samples to slide spectrogram window along the audio samples\n",
    "NUMBER_FFT    = 2048    # the number of FFT to execute within a single spectrogram window\n",
    "NUMBER_MELS   = 128     # the number of Mel-Spectrogram groups to split the frequency dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "\n",
    "This pipeline will go through a root directory and find all the audio files that exist and are of accepted format. Then, it will preprocess and augment the file by normalising it, trimming it, completing a frequency shift and exporting them as mp3 files to a new output directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class raw_file_pre_processing():\n",
    "    def __init__(self) -> None:\n",
    "        self.CLIP_LENGTH   = 5000   # only look at 5000 milliseconds of clip at the start of loaded audio file\n",
    "        self.BITRATE = \"32k\"        # all the samples are converted to bit rate of 32000 (Samples/Second)\n",
    "        self.labels = []\n",
    "        self.raw_dirs = {}\n",
    "        self.dataset = pd.DataFrame(columns=['Label', 'FileName', 'FileType', 'Directory'])\n",
    "        self.TARGET_FORMAT = 'mp3'\n",
    "        self.ACCEPTED_FORMAT = ['.mp3', '.flac', '.aiff', '.mp4', '.m4a', '.wav', '.ogg']\n",
    "\n",
    "        self.OUTPUT_DIR = os.path.join(DATASET_PATH, 'OUTPUT')\n",
    "        self.TRAIN_DIR = os.path.join(DATASET_PATH, 'TRAIN')\n",
    "        self.TEST_DIR = os.path.join(DATASET_PATH, 'TEST')\n",
    "        self.VALIDATION_DIR = os.path.join(DATASET_PATH, 'VALIDATION')\n",
    "\n",
    "        self.CLEAN_DIR_ = True\n",
    "\n",
    "        def clean_dir(self):\n",
    "            if os.path.exists(self.OUTPUT_DIR):\n",
    "                shutil.rmtree(self.OUTPUT_DIR)\n",
    "            if os.path.exists(self.TRAIN_DIR):\n",
    "                shutil.rmtree(self.TRAIN_DIR)\n",
    "            if os.path.exists(self.TEST_DIR):\n",
    "                shutil.rmtree(self.TEST_DIR)\n",
    "            if os.path.exists(self.VALIDATION_DIR):\n",
    "                shutil.rmtree(self.VALIDATION_DIR)\n",
    "        if self.CLEAN_DIR_: clean_dir(self)\n",
    "            \n",
    "\n",
    "    def get_raw_file_paths(self, directory):\n",
    "        print(f'Looking for files... acceptable formats include: {self.ACCEPTED_FORMAT}')\n",
    "        for root, dir, files in os.walk(directory):\n",
    "            if dir == [] and \"output\" not in str(root).split(\"/\")[1]:\n",
    "                tmp_lable = str(root).split(\"/\")[-1]\n",
    "                tmp_file_dir = []\n",
    "                for file in files:\n",
    "                    for ext in self.ACCEPTED_FORMAT:\n",
    "                        if ext in str(file):\n",
    "                            tmp_file_dir.append(os.path.join(root, file))\n",
    "                        \n",
    "                self.raw_dirs.update({tmp_lable:tmp_file_dir})\n",
    "\n",
    "        for key in self.raw_dirs:\n",
    "            print(f'FOUND: {key} -> {len(self.raw_dirs[key])}')\n",
    "\n",
    "    def audio_preprocessing(self, TRIM_AUDIO:bool = False, NORM_AUDIO:bool = False, TRAIN_TEST_SPLIT:bool = False):\n",
    "        print('\\nConvering audio files....')\n",
    "        if not os.path.exists(self.OUTPUT_DIR):\n",
    "            os.makedirs(self.OUTPUT_DIR)\n",
    "\n",
    "\n",
    "        for key, item in self.raw_dirs.items():\n",
    "            print(f'Converting {key} data ->> ...')\n",
    "            tmp_dir_key = os.path.join(self.OUTPUT_DIR, key)\n",
    "            if not os.path.exists(tmp_dir_key):\n",
    "                os.makedirs(tmp_dir_key)\n",
    "\n",
    "            for dir in item:\n",
    "                try:\n",
    "                    # read file\n",
    "                    tmp_file_name = str(dir).split(\"/\")[-1].split('.')[0]\n",
    "                    raw_sound = AudioSegment.from_file(dir, format=dir.split('.')[-1])\n",
    "\n",
    "                    if NORM_AUDIO:\n",
    "                        # normalise file\n",
    "                        raw_sound = effects.normalize(raw_sound)\n",
    "\n",
    "                    # trim file\n",
    "                    if TRIM_AUDIO:\n",
    "                        arr_split_file = [raw_sound[idx:idx + self.CLIP_LENGTH] for idx in range(0, len(raw_sound), self.CLIP_LENGTH)]             \n",
    "                        for count_sample, sample in enumerate(arr_split_file):\n",
    "                            # repeat if duration < 5s\n",
    "                            if len(sample) < self.CLIP_LENGTH:\n",
    "                                silence = AudioSegment.silent(duration=((self.CLIP_LENGTH-len(sample))))\n",
    "                                sample = sample + silence  # Adding silence after the audio\n",
    "\n",
    "                            # export raw file\n",
    "                            tmp_raw_new_dir = os.path.join(tmp_dir_key, tmp_file_name + '_raw_trim_sample_' + str(count_sample) + '.' + self.TARGET_FORMAT)\n",
    "                            sample.export(tmp_raw_new_dir, format=self.TARGET_FORMAT, bitrate=self.BITRATE)\n",
    "\n",
    "                            new_row = pd.Series({\"Label\": key,\n",
    "                                        \"FileName\": tmp_file_name + '_raw_trim_sample_' + str(count_sample) + '.' + self.TARGET_FORMAT,\n",
    "                                        \"FileType\": self.TARGET_FORMAT,\n",
    "                                        \"Directory\": tmp_raw_new_dir})\n",
    "                            self.dataset = pd.concat([self.dataset, new_row.to_frame().T], ignore_index=True)\n",
    "                    else:\n",
    "                        tmp_raw_new_dir = os.path.join(tmp_dir_key, tmp_file_name + '_raw_' + '.' + self.TARGET_FORMAT)\n",
    "                        raw_sound.export(tmp_raw_new_dir, format=self.TARGET_FORMAT, bitrate=self.BITRATE)\n",
    "\n",
    "                        new_row = pd.Series({\"Label\": key,\n",
    "                                    \"FileName\": tmp_file_name + '_raw_' + '.' + self.TARGET_FORMAT,\n",
    "                                    \"FileType\": self.TARGET_FORMAT,\n",
    "                                    \"Directory\": tmp_raw_new_dir})\n",
    "                        self.dataset = pd.concat([self.dataset, new_row.to_frame().T], ignore_index=True)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "        if TRAIN_TEST_SPLIT:\n",
    "            print(f'\\nSplitting data into subdirectories Train, Test and Validation...')\n",
    "            TRAIN_SPLIT = 0.8\n",
    "            VALIDATION_SPLIT = 0.1\n",
    "\n",
    "            if not os.path.exists(self.TRAIN_DIR):\n",
    "                os.mkdir(self.TRAIN_DIR)\n",
    "            if not os.path.exists(self.TEST_DIR):\n",
    "                os.mkdir(self.TEST_DIR)\n",
    "            if not os.path.exists(self.VALIDATION_DIR):\n",
    "                os.mkdir(self.VALIDATION_DIR)\n",
    "\n",
    "\n",
    "            dict_keys = self.dataset['Label'].value_counts().to_dict()\n",
    "            for key, item in dict_keys.items():\n",
    "                if not os.path.exists(os.path.join(self.TRAIN_DIR, key)):\n",
    "                    os.mkdir(os.path.join(self.TRAIN_DIR, key))\n",
    "                if not os.path.exists(os.path.join(self.TEST_DIR, key)):\n",
    "                    os.mkdir(os.path.join(self.TEST_DIR, key))\n",
    "                if not os.path.exists(os.path.join(self.VALIDATION_DIR, key)):\n",
    "                    os.mkdir(os.path.join(self.VALIDATION_DIR, key))\n",
    "\n",
    "                if item % 2 == 0:\n",
    "                    train_split_count_ = int(round(item*TRAIN_SPLIT,0))\n",
    "                else:\n",
    "                    train_split_count_ = int(round(item*TRAIN_SPLIT,0) -1)\n",
    "                validation_split_count_ = int(round(item*VALIDATION_SPLIT,0))\n",
    "\n",
    "                tmp_train_dirs = self.dataset.loc[self.dataset['Label'] == key][:train_split_count_]['Directory'].to_list()\n",
    "                tmp_validation_dirs = self.dataset.loc[self.dataset['Label'] == key][train_split_count_:train_split_count_+validation_split_count_]['Directory'].to_list()\n",
    "                tmp_test_dirs = self.dataset.loc[self.dataset['Label'] == key][train_split_count_+validation_split_count_:]['Directory'].to_list()\n",
    "\n",
    "                for i in tmp_train_dirs:\n",
    "                    os.replace(i, i.replace('OUTPUT', 'TRAIN'))\n",
    "                for i in tmp_validation_dirs:\n",
    "                    os.replace(i, i.replace('OUTPUT', 'VALIDATION'))\n",
    "                for i in tmp_test_dirs:\n",
    "                    os.replace(i, i.replace('OUTPUT', 'TEST'))\n",
    "\n",
    "            shutil.rmtree(self.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for files... acceptable formats include: ['.mp3', '.flac', '.aiff', '.mp4', '.m4a', '.wav', '.ogg']\n",
      "FOUND: jabwar -> 78\n",
      "FOUND: wiltur -> 76\n",
      "FOUND: sheowl -> 128\n",
      "FOUND: brant -> 135\n",
      "FOUND: spodov -> 107\n",
      "\n",
      "Convering audio files....\n",
      "Converting jabwar data ->> ...\n",
      "Converting wiltur data ->> ...\n",
      "Converting sheowl data ->> ...\n",
      "Converting brant data ->> ...\n",
      "Converting spodov data ->> ...\n",
      "\n",
      "Splitting data into subdirectories Train, Test and Validation...\n"
     ]
    }
   ],
   "source": [
    "data_preprocessing_pipeline = raw_file_pre_processing()\n",
    "\n",
    "data_preprocessing_pipeline.get_raw_file_paths(DATASET_PATH)\n",
    "dataset = data_preprocessing_pipeline.audio_preprocessing(TRIM_AUDIO=True, NORM_AUDIO=True, TRAIN_TEST_SPLIT=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melspectrogram Pipeline\n",
    "\n",
    "This pipeline will get all the new mp3 files, convert them to tfio tensors, convert to spectrograms, convert again to mel-spectrograms, then congert to db scale mel-spectrograms. It will then save the tensors as .pt files which can be read again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mel_spectrogram_pipeline():\n",
    "    def __init__(self) -> None:\n",
    "        self.target_dir = ''\n",
    "        self.labels = []\n",
    "        self.augmented_dirs = {}\n",
    "        self.OUTPUT_DIR = os.path.join(DATASET_PATH, 'output')\n",
    "        self.ACCEPTED_FORMAT = '.mp3'\n",
    "        self.TENSOR_OUTPUT_DIR = os.path.join(DATASET_PATH, 'tensors')\n",
    "            \n",
    "    def get_preprocessed_files(self):\n",
    "        print(f'Looking for preprocessed files in output dir')\n",
    "        for root, dir, files in os.walk(self.OUTPUT_DIR):\n",
    "            if dir == [] and \"output\" not in str(root).split(\"/\")[1]:\n",
    "                tmp_lable = str(root).split(\"/\")[-1]\n",
    "                tmp_file_dir = []\n",
    "                for file in files:\n",
    "                    if self.ACCEPTED_FORMAT in str(file):\n",
    "                        tmp_file_dir.append(os.path.join(root, file))\n",
    "                        \n",
    "                self.augmented_dirs.update({tmp_lable:tmp_file_dir})\n",
    "\n",
    "        for key in self.augmented_dirs:\n",
    "            print(f'FOUND: {key} -> {len(self.augmented_dirs[key])}')\n",
    "\n",
    "    def generate_mel_spectrograms(self):\n",
    "        print(f'\\nGenerating melspectrograms... \\n')\n",
    "        for key, item in self.augmented_dirs.items():\n",
    "            for dir in item:\n",
    "                tmp_dir_key = f'{self.TENSOR_OUTPUT_DIR}/{key}/'\n",
    "                if not os.path.exists(tmp_dir_key):\n",
    "                    os.makedirs(tmp_dir_key)\n",
    "\n",
    "                tmp_audio = tfio.audio.AudioIOTensor(dir)\n",
    "                tmp_audio_t = tmp_audio.to_tensor()\n",
    "\n",
    "                # Convert to spectrogram\n",
    "                spectrogram = tfio.audio.spectrogram(\n",
    "                    tmp_audio_t, nfft=2048, window=512, stride=128)\n",
    "\n",
    "                # # Convert to mel-spectrogram\n",
    "                mel_spectrogram = tfio.audio.melscale(\n",
    "                    spectrogram, rate=16000, mels=128, fmin=16, fmax=4096*3)\n",
    "\n",
    "                # Convert to db scale mel-spectrogram\n",
    "                dbscale_mel_spectrogram = tfio.audio.dbscale(\n",
    "                    mel_spectrogram, top_db=80)\n",
    "\n",
    "                # if FREQ_SHIFT:\n",
    "                #     #frequency shift\n",
    "                #     octave = -0.5\n",
    "                #     new_sample_rate = int(sample.frame_rate * (2.0 ** octave))\n",
    "                #     freg_shift_sample = sample._spawn(sample.raw_data, overrides={'frame_rate': new_sample_rate})\n",
    "                    \n",
    "                #     # export freq_shift file\n",
    "                #     tmp_freq_new_dir = os.path.join(tmp_dir_key, tmp_file_name + '_freq_' + str(count_sample) + '.' + self.TARGET_FORMAT)\n",
    "                #     freg_shift_sample.export(tmp_freq_new_dir, format=self.TARGET_FORMAT, bitrate=self.BITRATE)\n",
    "                #     tmp_arr_mp3_dir.append(tmp_freq_new_dir)\n",
    "\n",
    "                # Freq masking\n",
    "                freq_mask = tfio.audio.freq_mask(dbscale_mel_spectrogram, param=10)\n",
    "                print(freq_mask)\n",
    "                input()\n",
    "\n",
    "                # Time masking\n",
    "                time_mask = tfio.audio.time_mask(dbscale_mel_spectrogram, param=10)\n",
    "\n",
    "                torch.save(mel_spectrogram, f'{self.TENSOR_OUTPUT_DIR}/{key}/{dir.split(\"/\")[-1].split(\".\")[0]}_mel_spectrogram.pt')\n",
    "                torch.save(dbscale_mel_spectrogram, f'{self.TENSOR_OUTPUT_DIR}/{key}/{dir.split(\"/\")[-1].split(\".\")[0]}_dbscale_mel_spectrogram.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for preprocessed files in output dir\n",
      "FOUND: jabwar -> 1762\n",
      "FOUND: wiltur -> 2516\n",
      "FOUND: sheowl -> 2092\n",
      "FOUND: brant -> 1992\n",
      "FOUND: spodov -> 1636\n",
      "\n",
      "Generating melspectrograms... \n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__RandomUniformInt_device_/job:localhost/replica:0/task:0/device:CPU:0}} Need minval < maxval, got 0 >= -5 [Op:RandomUniformInt]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m data_spectro_pipeline \u001b[38;5;241m=\u001b[39m mel_spectrogram_pipeline()\n\u001b[1;32m      3\u001b[0m data_spectro_pipeline\u001b[38;5;241m.\u001b[39mget_preprocessed_files()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdata_spectro_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_mel_spectrograms\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mmel_spectrogram_pipeline.generate_mel_spectrograms\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m dbscale_mel_spectrogram \u001b[38;5;241m=\u001b[39m tfio\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mdbscale(\n\u001b[1;32m     46\u001b[0m     mel_spectrogram, top_db\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Freq masking\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m freq_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtfio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreq_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbscale_mel_spectrogram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(freq_mask)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28minput\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow_io/python/ops/audio_ops.py:334\u001b[0m, in \u001b[0;36mfreq_mask\u001b[0;34m(input, param, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m freq_max \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(\u001b[38;5;28minput\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    333\u001b[0m f \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(shape\u001b[38;5;241m=\u001b[39m(), minval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, maxval\u001b[38;5;241m=\u001b[39mparam, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m--> 334\u001b[0m f0 \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq_max\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m indices \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(tf\u001b[38;5;241m.\u001b[39mrange(freq_max), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    338\u001b[0m condition \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlogical_and(\n\u001b[1;32m    339\u001b[0m     tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mgreater_equal(indices, f0), tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mless(indices, f0 \u001b[38;5;241m+\u001b[39m f)\n\u001b[1;32m    340\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__RandomUniformInt_device_/job:localhost/replica:0/task:0/device:CPU:0}} Need minval < maxval, got 0 >= -5 [Op:RandomUniformInt]"
     ]
    }
   ],
   "source": [
    "data_spectro_pipeline = mel_spectrogram_pipeline()\n",
    "\n",
    "data_spectro_pipeline.get_preprocessed_files()\n",
    "data_spectro_pipeline.generate_mel_spectrograms()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
