{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview\n",
    "\n",
    "Author: stephankokkas\n",
    "\n",
    "This notebook defines a pipeline that tasks an input directory of audio files and converts them to images using mel-spectrogram transofrmation and preprocessing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-25 20:11:55.449501: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version     :  3.10.0\n",
      "TensorFlow Version :  2.11.0\n"
     ]
    }
   ],
   "source": [
    "# disable warnings to tidy up output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# some basic libraries \n",
    "from platform import python_version\n",
    "#import pandas as pd\n",
    "#import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# plot support\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tensorflow support\n",
    "import tensorflow as tf\n",
    "#import tensorflow_transform as tft\n",
    "import tensorflow_io as tfio\n",
    "#from tensorflow.contrib.framework.python.ops import audio_ops\n",
    "\n",
    "# scipy\n",
    "import scipy\n",
    "from pydub import AudioSegment, effects\n",
    "\n",
    "# turn off tensorflow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# turn off absl warnings\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "# print system information\n",
    "print('Python Version     : ', python_version())\n",
    "print('TensorFlow Version : ', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below code adapted from:\n",
    "# https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(123)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "random.seed(123)\n",
    "\n",
    "# The below set_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set system parameters\n",
    "DATASET_PATH  = '/Users/stephankokkas/Downloads/birdclef2022/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "\n",
    "This pipeline will go through a root directory and find all the audio files that exist and are of accepted format. Then, depending on the params set, it with normalise, trim and split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class raw_file_pre_processing():\n",
    "    def __init__(self) -> None:\n",
    "        self.CLIP_LENGTH   = 5000   # only look at 5000 milliseconds of clip at the start of loaded audio file\n",
    "        self.BITRATE = \"32k\"        # all the samples are converted to bit rate of 32000 (Samples/Second)\n",
    "        self.labels = []\n",
    "        self.raw_dirs = {}\n",
    "        self.dataset = pd.DataFrame(columns=['Label', 'FileName', 'FileType', 'Directory'])\n",
    "        self.TARGET_FORMAT = 'mp3'\n",
    "        self.ACCEPTED_FORMAT = ['.mp3', '.flac', '.aiff', '.mp4', '.m4a', '.wav', '.ogg']\n",
    "\n",
    "        self.OUTPUT_DIR = os.path.join(DATASET_PATH, 'OUTPUT')\n",
    "        self.TRAIN_DIR = os.path.join(DATASET_PATH, 'TRAIN')\n",
    "        self.TEST_DIR = os.path.join(DATASET_PATH, 'TEST')\n",
    "        self.VALIDATION_DIR = os.path.join(DATASET_PATH, 'VALIDATION')\n",
    "\n",
    "        self.CLEAN_DIR_ = True\n",
    "\n",
    "        self.TRAIN_SPLIT = 0.8\n",
    "        self.VALIDATION_SPLIT = 0.1\n",
    "\n",
    "        def clean_dir(self):\n",
    "            if os.path.exists(self.OUTPUT_DIR):\n",
    "                shutil.rmtree(self.OUTPUT_DIR)\n",
    "            if os.path.exists(self.TRAIN_DIR):\n",
    "                shutil.rmtree(self.TRAIN_DIR)\n",
    "            if os.path.exists(self.TEST_DIR):\n",
    "                shutil.rmtree(self.TEST_DIR)\n",
    "            if os.path.exists(self.VALIDATION_DIR):\n",
    "                shutil.rmtree(self.VALIDATION_DIR)\n",
    "        if self.CLEAN_DIR_: clean_dir(self)\n",
    "            \n",
    "\n",
    "    def get_raw_file_paths(self, directory):\n",
    "        print(f'Looking for files... acceptable formats include: {self.ACCEPTED_FORMAT}')\n",
    "        for root, dir, files in os.walk(directory):\n",
    "            if dir == [] and \"output\" not in str(root).split(\"/\")[1]:\n",
    "                tmp_lable = str(root).split(\"/\")[-1]\n",
    "                tmp_file_dir = []\n",
    "                for file in files:\n",
    "                    for ext in self.ACCEPTED_FORMAT:\n",
    "                        if ext in str(file):\n",
    "                            tmp_file_dir.append(os.path.join(root, file))\n",
    "                        \n",
    "                self.raw_dirs.update({tmp_lable:tmp_file_dir})\n",
    "\n",
    "        for key in self.raw_dirs:\n",
    "            print(f'FOUND: {key} -> {len(self.raw_dirs[key])}')\n",
    "\n",
    "    def audio_preprocessing(self, TRIM_AUDIO:bool = False, NORM_AUDIO:bool = False, TRAIN_TEST_SPLIT:bool = False):\n",
    "        print('\\nConvering audio files....')\n",
    "        if not os.path.exists(self.OUTPUT_DIR):\n",
    "            os.makedirs(self.OUTPUT_DIR)\n",
    "\n",
    "\n",
    "        for key, item in self.raw_dirs.items():\n",
    "            print(f'Converting {key} data ->> ...')\n",
    "            tmp_dir_key = os.path.join(self.OUTPUT_DIR, key)\n",
    "            if not os.path.exists(tmp_dir_key):\n",
    "                os.makedirs(tmp_dir_key)\n",
    "\n",
    "            for dir in item:\n",
    "                try:\n",
    "                    # read file\n",
    "                    tmp_file_name = str(dir).split(\"/\")[-1].split('.')[0]\n",
    "                    raw_sound = AudioSegment.from_file(dir, format=dir.split('.')[-1])\n",
    "\n",
    "                    if NORM_AUDIO:\n",
    "                        # normalise file\n",
    "                        raw_sound = effects.normalize(raw_sound)\n",
    "\n",
    "                    # trim file\n",
    "                    if TRIM_AUDIO:\n",
    "                        arr_split_file = [raw_sound[idx:idx + self.CLIP_LENGTH] for idx in range(0, len(raw_sound), self.CLIP_LENGTH)]             \n",
    "                        for count_sample, sample in enumerate(arr_split_file):\n",
    "                            # padding audio < 5s\n",
    "                            if len(sample) < self.CLIP_LENGTH:\n",
    "                                silence = AudioSegment.silent(duration=((self.CLIP_LENGTH-len(sample))))\n",
    "                                sample = sample + silence  # Adding silence after the audio\n",
    "\n",
    "                            # export raw file\n",
    "                            tmp_raw_new_dir = os.path.join(tmp_dir_key, tmp_file_name + '_raw_trim_sample_' + str(count_sample) + '.' + self.TARGET_FORMAT)\n",
    "                            sample.export(tmp_raw_new_dir, format=self.TARGET_FORMAT, bitrate=self.BITRATE)\n",
    "\n",
    "                            new_row = pd.Series({\"Label\": key,\n",
    "                                        \"FileName\": tmp_file_name + '_raw_trim_sample_' + str(count_sample) + '.' + self.TARGET_FORMAT,\n",
    "                                        \"FileType\": self.TARGET_FORMAT,\n",
    "                                        \"Directory\": tmp_raw_new_dir})\n",
    "                            self.dataset = pd.concat([self.dataset, new_row.to_frame().T], ignore_index=True)\n",
    "                    else:\n",
    "                        tmp_raw_new_dir = os.path.join(tmp_dir_key, tmp_file_name + '_raw_' + '.' + self.TARGET_FORMAT)\n",
    "                        raw_sound.export(tmp_raw_new_dir, format=self.TARGET_FORMAT, bitrate=self.BITRATE)\n",
    "\n",
    "                        new_row = pd.Series({\"Label\": key,\n",
    "                                    \"FileName\": tmp_file_name + '_raw_' + '.' + self.TARGET_FORMAT,\n",
    "                                    \"FileType\": self.TARGET_FORMAT,\n",
    "                                    \"Directory\": tmp_raw_new_dir})\n",
    "                        self.dataset = pd.concat([self.dataset, new_row.to_frame().T], ignore_index=True)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "        if TRAIN_TEST_SPLIT:\n",
    "            print(f'\\nSplitting data into sub-directories Train, Test and Validation...')\n",
    "\n",
    "            if not os.path.exists(self.TRAIN_DIR):\n",
    "                os.mkdir(self.TRAIN_DIR)\n",
    "            if not os.path.exists(self.TEST_DIR):\n",
    "                os.mkdir(self.TEST_DIR)\n",
    "            if not os.path.exists(self.VALIDATION_DIR):\n",
    "                os.mkdir(self.VALIDATION_DIR)\n",
    "\n",
    "\n",
    "            dict_keys = self.dataset['Label'].value_counts().to_dict()\n",
    "            for key, item in dict_keys.items():\n",
    "                if not os.path.exists(os.path.join(self.TRAIN_DIR, key)):\n",
    "                    os.mkdir(os.path.join(self.TRAIN_DIR, key))\n",
    "                if not os.path.exists(os.path.join(self.TEST_DIR, key)):\n",
    "                    os.mkdir(os.path.join(self.TEST_DIR, key))\n",
    "                if not os.path.exists(os.path.join(self.VALIDATION_DIR, key)):\n",
    "                    os.mkdir(os.path.join(self.VALIDATION_DIR, key))\n",
    "\n",
    "                if item % 2 == 0:\n",
    "                    train_split_count_ = int(round(item*self.TRAIN_SPLIT,0))\n",
    "                else:\n",
    "                    train_split_count_ = int(round(item*self.TRAIN_SPLIT,0) -1)\n",
    "                validation_split_count_ = int(round(item*self.VALIDATION_SPLIT,0))\n",
    "\n",
    "                tmp_train_dirs = self.dataset.loc[self.dataset['Label'] == key][:train_split_count_]['Directory'].to_list()\n",
    "                tmp_validation_dirs = self.dataset.loc[self.dataset['Label'] == key][train_split_count_:train_split_count_+validation_split_count_]['Directory'].to_list()\n",
    "                tmp_test_dirs = self.dataset.loc[self.dataset['Label'] == key][train_split_count_+validation_split_count_:]['Directory'].to_list()\n",
    "\n",
    "                for i in tmp_train_dirs:\n",
    "                    os.replace(i, i.replace('OUTPUT', 'TRAIN'))\n",
    "                for i in tmp_validation_dirs:\n",
    "                    os.replace(i, i.replace('OUTPUT', 'VALIDATION'))\n",
    "                for i in tmp_test_dirs:\n",
    "                    os.replace(i, i.replace('OUTPUT', 'TEST'))\n",
    "\n",
    "            shutil.rmtree(self.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for files... acceptable formats include: ['.mp3', '.flac', '.aiff', '.mp4', '.m4a', '.wav', '.ogg']\n",
      "FOUND: jabwar -> 78\n",
      "FOUND: wiltur -> 76\n",
      "FOUND: sheowl -> 128\n",
      "FOUND: brant -> 135\n",
      "FOUND: spodov -> 107\n",
      "\n",
      "Convering audio files....\n",
      "Converting jabwar data ->> ...\n",
      "Converting wiltur data ->> ...\n",
      "Converting sheowl data ->> ...\n",
      "Converting brant data ->> ...\n",
      "Converting spodov data ->> ...\n",
      "\n",
      "Splitting data into subdirectories Train, Test and Validation...\n"
     ]
    }
   ],
   "source": [
    "data_preprocessing_pipeline = raw_file_pre_processing()\n",
    "\n",
    "data_preprocessing_pipeline.get_raw_file_paths(DATASET_PATH)\n",
    "dataset = data_preprocessing_pipeline.audio_preprocessing(TRIM_AUDIO=True, NORM_AUDIO=True, TRAIN_TEST_SPLIT=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melspectrogram Pipeline\n",
    "\n",
    "This pipeline will get all the new mp3 files, convert them to tfio tensors, convert to spectrograms, convert again to mel-spectrograms, then congert to db scale mel-spectrograms. It will then save the tensors as .pt files which can be read again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mel_spectrogram_pipeline():\n",
    "    def __init__(self) -> None:\n",
    "        self.target_dir = ''\n",
    "        self.labels = []\n",
    "        self.augmented_dirs = {}\n",
    "        self.OUTPUT_DIR = {}\n",
    "        self.ACCEPTED_FORMAT = '.mp3'\n",
    "        self.TENSOR_OUTPUT_DIR = os.path.join(DATASET_PATH, 'tensors')\n",
    "\n",
    "        self.NFFT = 512\n",
    "        self.WINDOW = 512\n",
    "        self.STRIDE = 256\n",
    "        self.RATE = 16000\n",
    "        self.MELS = 128\n",
    "        self.FMIN = 0\n",
    "        self.FMAX = 8000\n",
    "        self.TOP_DB = 80\n",
    "\n",
    "    \n",
    "    def clean_dirs(self):\n",
    "        print('Cleaning tensor directory')\n",
    "        if os.path.exists(self.TENSOR_OUTPUT_DIR):\n",
    "            shutil.rmtree(self.TENSOR_OUTPUT_DIR)\n",
    "\n",
    "    def get_output_dir(self):\n",
    "        print('Finding all pre-processed files')\n",
    "        for root, dir, files in os.walk(DATASET_PATH):\n",
    "            if \"TRAIN\" not in dir:\n",
    "                if \"OUTPUT\" not in dir:\n",
    "                    raise ValueError('Cant find any directories with pre-processed data. Looking for OUTPUT or TRAIN, TEST, and VAIDATION')\n",
    "                else:\n",
    "                    self.OUTPUT_DIR.update({\"OUTPUT\": os.path.join(root, \"OUTPUT\")})\n",
    "            else:\n",
    "                if \"TEST\" in dir and \"VALIDATION\" in dir:\n",
    "                    self.OUTPUT_DIR.update({\"TRAIN\": os.path.join(root, \"TRAIN\")})\n",
    "                    self.OUTPUT_DIR.update({\"TEST\": os.path.join(root, \"TEST\")})\n",
    "                    self.OUTPUT_DIR.update({\"VALIDATION\": os.path.join(root, \"VALIDATION\")})\n",
    "            if not self.OUTPUT_DIR:\n",
    "                raise ValueError('Cant find any directories with pre-processed data. Looking for OUTPUT or TRAIN, TEST, and VAIDATION')\n",
    "\n",
    "            print(f'Found the following directories {self.OUTPUT_DIR}')\n",
    "            break\n",
    "            \n",
    "    def get_preprocessed_files(self):\n",
    "        self.get_output_dir()\n",
    "        \n",
    "        for key, item in self.OUTPUT_DIR.items():\n",
    "            for root, dir, files in os.walk(item):\n",
    "                if dir == []:\n",
    "                    tmp_lable = str(root).split(\"/\")[-1] + \"-\" + key\n",
    "                    tmp_file_dir = []\n",
    "                    for file in files:\n",
    "                        if self.ACCEPTED_FORMAT in str(file):\n",
    "                            tmp_file_dir.append(os.path.join(root, file))\n",
    "                            \n",
    "                    self.augmented_dirs.update({tmp_lable:tmp_file_dir})\n",
    "\n",
    "        for key in self.augmented_dirs:\n",
    "            print(f'FOUND: {key} -> {len(self.augmented_dirs[key])}')\n",
    "\n",
    "    def generate_mel_spectrograms(self, DB_SCALE:bool = False, MEL_SPECTRO:bool = False):\n",
    "        print(f'\\nGenerating melspectrogram and db-melspectrogram per mp3 file... \\n')\n",
    "        for key, item in self.augmented_dirs.items():\n",
    "            for dir in item:\n",
    "                tmp_dir_key = f'{self.TENSOR_OUTPUT_DIR}/{key}/'\n",
    "                if not os.path.exists(tmp_dir_key):\n",
    "                    os.makedirs(tmp_dir_key)\n",
    "\n",
    "                tmp_audio = tfio.audio.AudioIOTensor(dir)\n",
    "                tmp_audio_t = tmp_audio.to_tensor()\n",
    "\n",
    "                # Convert to spectrogram\n",
    "                spectrogram = tfio.audio.spectrogram(\n",
    "                    tmp_audio_t, nfft=self.NFFT, window=self.WINDOW, stride=self.STRIDE)\n",
    "\n",
    "                if MEL_SPECTRO:\n",
    "                    # # Convert to mel-spectrogram\n",
    "                    # ValueError: upper_edge_hertz must not be larger than the Nyquist frequency (sample_rate / 2)\n",
    "                    mel_spectrogram = tfio.audio.melscale(\n",
    "                        spectrogram, rate=self.RATE, mels=self.MELS, fmin=self.FMIN, fmax=self.FMAX)\n",
    "                    torch.save(mel_spectrogram, f'{self.TENSOR_OUTPUT_DIR}/{key}/{dir.split(\"/\")[-1].split(\".\")[0]}_raw_mel_spectrogram.pt')\n",
    "\n",
    "                if DB_SCALE:\n",
    "                    # Convert to db scale mel-spectrogram\n",
    "                    dbscale_mel_spectrogram = tfio.audio.dbscale(\n",
    "                        mel_spectrogram, top_db=self.TOP_DB)\n",
    "                    torch.save(dbscale_mel_spectrogram, f'{self.TENSOR_OUTPUT_DIR}/{key}/{dir.split(\"/\")[-1].split(\".\")[0]}_dbscale_raw_mel_spectrogram.pt')\n",
    "\n",
    "                print('Done')\n",
    "                break\n",
    "            break\n",
    "\n",
    "                # if FREQ_SHIFT:\n",
    "                #     #frequency shift\n",
    "                #     octave = -0.5\n",
    "                #     new_sample_rate = int(sample.frame_rate * (2.0 ** octave))\n",
    "                #     freg_shift_sample = sample._spawn(sample.raw_data, overrides={'frame_rate': new_sample_rate})\n",
    "                    \n",
    "                #     # export freq_shift file\n",
    "                #     tmp_freq_new_dir = os.path.join(tmp_dir_key, tmp_file_name + '_freq_' + str(count_sample) + '.' + self.TARGET_FORMAT)\n",
    "                #     freg_shift_sample.export(tmp_freq_new_dir, format=self.TARGET_FORMAT, bitrate=self.BITRATE)\n",
    "                #     tmp_arr_mp3_dir.append(tmp_freq_new_dir)\n",
    "\n",
    "                # # Freq masking\n",
    "                # freq_mask = tfio.audio.freq_mask(dbscale_mel_spectrogram, param=10)\n",
    "                # print(freq_mask)\n",
    "                # input()\n",
    "\n",
    "                # Time masking\n",
    "                # time_mask = tfio.audio.time_mask(dbscale_mel_spectrogram, param=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning tensor directory\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "mel_spectrogram_pipeline.get_output_dir() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m data_spectro_pipeline \u001b[38;5;241m=\u001b[39m mel_spectrogram_pipeline()\n\u001b[1;32m      3\u001b[0m data_spectro_pipeline\u001b[38;5;241m.\u001b[39mclean_dirs()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdata_spectro_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_preprocessed_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m data_spectro_pipeline\u001b[38;5;241m.\u001b[39mgenerate_mel_spectrograms(DB_SCALE\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, MEL_SPECTRO\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mmel_spectrogram_pipeline.get_preprocessed_files\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_preprocessed_files\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mOUTPUT_DIR\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m root, \u001b[38;5;28mdir\u001b[39m, files \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(item):\n",
      "\u001b[0;31mTypeError\u001b[0m: mel_spectrogram_pipeline.get_output_dir() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "data_spectro_pipeline = mel_spectrogram_pipeline()\n",
    "\n",
    "data_spectro_pipeline.clean_dirs()\n",
    "data_spectro_pipeline.get_preprocessed_files()\n",
    "data_spectro_pipeline.generate_mel_spectrograms(DB_SCALE=False, MEL_SPECTRO=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
